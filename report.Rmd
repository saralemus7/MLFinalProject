---
title: "Analyzing and Predicting the Relationships between Sesame Street Viewership and Test Scores among School Children"
author: "Angela Wang, QiHan Zhou, Matthew Murray, Michelle Mao, Sara Lemus, Sophie Dalldorf"
date: "12/9/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=F, message=F)
knitr::opts_chunk$set(fig.pos = "h", out.extra = "")
```

```{r loading-libraries}
library(foreign)
library(tidyverse)
library(e1071)
library(tree)
library(gbm)
library(randomForest)
library(caret)
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidyverse)
library(patchwork)
library(UBL)
library(scales)
library(glmnet)
library(kableExtra)
sesame <- read.dta("sesame.dta")
sesame <- sesame %>%
  mutate(site=factor(site)) %>%
  mutate(bodyDiff = postbody - prebody,
         letDiff = postlet - prelet,
         formDiff = postform - preform,
         numbDiff = postnumb - prenumb,
         relatDiff = postrelat - prerelat,
         clasfDiff = postclasf - preclasf)
sesame.sd <- sesame%>%
  mutate(sd_pBod = scale(prebody, center = TRUE, scale = TRUE),
         sd_plet = scale(prelet, center = TRUE, scale = TRUE),
         sd_pform = scale(preform, center = TRUE, scale = TRUE),
         sd_pnumb = scale(prenumb, center = TRUE, scale = TRUE),
         sd_prelat = scale(prerelat, center = TRUE, scale = TRUE),
         sd_pclasf = scale(preclasf, center = TRUE, scale = TRUE),
         sd_peabody = scale(peabody, center = TRUE, scale = TRUE), 
         sd_age = scale(age, center =TRUE, scale = TRUE),
         male=if_else(sex==1, 1, 0),
         female=if_else(sex==2, 1, 0))
```

# Introduction


# Methodology 

## Research Question 1

We decided to utilize three different types of models to gain inference and predict the difference in test scores that occurs after subjects watch Sesame Street: (1) least-squares regression model (2) ridge regression model and (3) regression tree model. There are six different test scores in the data set, so we fit three models for each difference in test score variable. Each model took the following variables as covariates: ``site``, ``sex``, ``viewcat``, ``setting``, ``viewenc``. Before creating these models, we factored those variables to encode them as categoricals. Initially, our least-squares and ridge regression models also took into account all possible two-way interaction terms between the variables. However, we noticed that both the Akaike Information Criterion (AIC) and adjusted-$R^2$ values were higher for models that did not include any interaction effects. That reason, coupled with the lack of apparent interaction effects in our exploratory data analysis and the fact that we wanted our linear models to be very interpretable, is why we decided not to include any interaction effects in our final linear models. 

One problem that we envisioned when evaluating and comparing the different models is that the tests are scored on different scales. For example, the scores for the test on knowledge of body parts (noted by ``bodyDiff``) range from 0-32, while those of the test on letters (noted by ``letDiff``) range from 0-58. To be able to aptly compare the mean squared error (MSE) between models, we also decided to convert each response variable to the same range. More specifically, we scaled each variable to the arbitrary range [0, 30]. Lastly, we randomly split the data between testing and training, using 70% of the data for training and 30% of the data for testing.

We first decided to use least-squares regression models due to the fact that they provide apt inference into the relationship between the covariates and the response variable. Thereafter, we decided to use a ridge regression model due to the fact that ridge regression often provides performance improvements by shrinking slope coefficients. While shrinkage may introduce bias to a model, it decreases the variance and increases the precision of the slope coefficient estimates. The shrinkage is achieved by applying a shrinkage parameter, $\lambda$, to the Euclidean norm of a slope coefficient. Doing so slightly increases the bias of our model (as least-squares regression coefficient estimates are unbiased) but can also significantly decrease the variance (and increase the precision) of our regression coefficients. This slight increase in bias but significant decrease in variance usually decreases the MSE of a model.

We tuned our $\lambda$ parameter using 10-fold cross validation. More specifically, we computed the cross-validation error rate for our model for a grid of $\lambda$ values. Thereafter, we selected the $\lambda$ value for which the cross-validation error is the smallest. 

Initially, we were inclined to use least absolute shrinkage and selection operator (LASSO) regression. The main advantage of LASSO regression over ridge regression is that LASSO regression performs variable selection by setting the slope coefficients of inert predictors to 0. The reason why we initially thought that LASSO regression would work better than ridge regression is that in most of our linear models, only a small subset of variables are significant at a 95% significance level.  However, our ridge regression models performed marginally better than our LASSO regression models, so for this reason, we decided to report the MSEâ€™s for the ridge regression models. 

~Talk about why we chose regression tree models~

In the context of a regression tree, the deviance is simply the sum of the squared errors.

## Research Question 2
In order to address our second research question, predicting whether a child came from an disadvantaged background or not based on their pretest scores and demographic information, we utilized a support vector machine (SVM). Our model uses the sex, age, and all pretest score variables to predict our response variable, site. Since our response variable is a categorical variable, a SVM is a valid choice to answer our research question. We also implemented a classification tree and a logistic model to answer this question as well; however, these models did not perform as well on our data (See Appendix for more details). Thus, a SVM was the most appropriate model choice for our research goals. Our full model formula is:
$$
svm(site \mathtt{\sim} female + male + age + bodyparts_{pretest}+ letters_{pretest} + 
\\ forms_{pretest} +numbers_{pretest} + relationalterms_{pretest} + classificationskills_{pretest} )
$$

We split our data into a 70% training set and 30% testing set and analyzed the performance of our model on the test set. Since we are interested in high predictive power, we implemented a variety of different methods. Standardizing the predictor variables in SVM and encoding categorical variables has been shown to improve performance for SVMs [1]. Thus, we used the standardized forms of the continuous variables in our model and encoded the female and male variables. We did indeed observe a small improvement in prediction accuracy across all models. However, one problem that particularly piqued our interests is that we are making no predictions for classes 4 or 5 for site. This could be due to sites 4 & 5 having a smaller number of observations than the other classes. Thus, we used the class weight formula below to assign weights to each class and specify "one versus one" comparison, which has been suggested to yield better prediction than "one versus all" [2].

$$
w_j =\frac{n}{kn_j} , \text{ n is total number of data points, k is number of classes, }n_j \text{ is the number of data in class j}
$$

We tested our model using linear, radial, sigmoid, and polynomial kernels and compared the predictive accuracy between these models. After the class weight assignment, the SVM models began to make prediction on class 4&5. However, doing so came at the cost of overall accuracy. Thus, more of the other observations are being misclassified, but the few observations of sites 4&5 are correctly classified. To remedy this issue, we began to experiment with the class weights and increase the sites 4&5 weights by roughly 0.5 until we reached the highest predictive power. The model with the highest accuracy was the linear kernel SVM with weights 0.8 on site 1, 0.87 on site 2, 0.75 on site 3, 1.5 on site 4, and 3 on site 5 and a cross validation selected cost parameter of 0.1. Since this model has the highest predictive accuracy of all others that we had tried, we settled on this model as our final model. There are no explicit tests for SVM model diagnostics; however, there seems to be no major issue with our model fit.

# Results

## Research Question 1 

Linear Regression Models

$$ bodyDiff = 16.43 \ + \ 2.45 \ (site3) \ + \ 2.97 \ (site4) \ + \ 3.30 \ (site5) $$
$$ letDiff = 10.52 \ + \ 3.09 \ (site2) \ - \ 2.86 \ (site3) \ + 4.85 \ (viewcat3) \ + 4.77 \ (viewcat4) $$
$$ formDiff = 16.04 \ + \ 2.48 \ (viewcat4)$$
$$ numbDiff = 15.39 \ + \ 2.56 \ (site2) \ + \ 3.20 \ (viewcat3) \ + \ 3.64 \ (viewcat4)$$
$$ relatDiff = 18.49 \ + \ 2.42 \ (site4) \ + \ 2.96 \ (viewcat4) $$
$$ clasfDiff = 10.95 \ + \ 4.06 \ (viewcat4) $$

```{r}

df1 <- data.frame(Response = c("bodyDiff", "letDiff", "formDiff", "numbDiff", "relatDiff", "clasfDiff"), 
Least.Squares.Regression.Test.MSE = c(24.01, 14.31, 13.26, 15.51, 18.94, 48.40), Ridge.Regression.Test.MSE = c(21.61, 14.20, 12.83, 14.63, 19.86, 44.26), Regression.Tree.Test.MSE = c(20.60, 15.41, 14.92, 15.91, 19.89, 45.53))

table <- kable(df1, caption = "Test Metrics", booktabs=T)
kable_styling(table, bootstrap_options = "striped", full_width = F, latex_options = "HOLD_position")
```

From the results of the above table, one can see that the ridge regression models have the best performance, although the performance of the regression trees are very similar . For each response variable except ``bodyDiff``, the ridge regression model reports the lowest test MSE, although the difference in performance between the ridge regression and the regression tree is very marginal. 

```{r classMod-accuracy-tab}
classfMod.tab <- data.frame(Model = c("Linear Kernel SVM",
                                      "Radial Kernel SVM",
                                      "Sigmoid Kernel SVM",
                                      "Polynomial Kernel SVM",
                                      "Random Forest"),
                            Accuracy = c(0.4028, 0.3611, 
                                         0.2639, 0.2917, 
                                         0.4444))
all.accuracy <- kable(classfMod.tab, caption = "Classification Model Accuracy Table", booktabs=T)
kable_styling(all.accuracy, bootstrap_options = "striped", full_width = F, latex_options = "HOLD_position")
```

## Research Question 2

```{r test-train-split}
set.seed(3241)

n <- nrow(sesame)
train.index <- sample(1:n, size = floor(0.7*n), replace=FALSE)
train.data <- sesame.sd[train.index,]
test.data <- sesame.sd[-train.index,]
```

```{r classWeight-assigning}
set.seed(315)
costs <- c(0.001, 0.01, 0.1, 1, 5, 10, 100)
total.weight <- 60+55+64+43+18
weight.1 <- total.weight/(5*60)
weight.2 <- total.weight/(5*55)
weight.3 <- total.weight/(5*64)  
weight.4 <- total.weight/(5*43)  
weight.5 <- total.weight/(5*18)  

#increase the weight of class 4 & 5 by a little bit over 0.4(chosen arbitraily)
weight.4 <- 1.5
weight.5 <- 3

linear.weighted <- tune(svm, site~female+ male + sd_age+sd_pBod+sd_plet+sd_pform + sd_pnumb+sd_prelat+sd_pclasf+sd_peabody, 
                    data=train.data, kernel="linear",
                    ranges=list(cost=costs),
                    class.weights=c("1"=weight.1,
                                    "2"=weight.2,
                                    "3"=weight.3,
                                    "4"=weight.4,
                                    "5"=weight.5),
                    class.type="one.versus.one")

linear.w.cm <- table(true=test.data[, "site"],
                          pred=predict(linear.weighted$best.model, newdata=test.data))
```

```{r, include = F}
confusionMatrix(linear.w.cm)
```

As evidenced in the table below, the class weighted linear kernel SVM has a prediction accuracy of 0.403 on our test data with a confidence interval of 0.289 to 0.525. From the confusion matrix, we can also see that our model predicts well for classes 2 and 3 and struggles slightly with clases 1,4, and 5. Specifically, our model has tends to predict class 3 often when the true class is 4, and it tends to predict classes 3 and 5 as commonly as class 1 when the true class is 1.

```{r}
ressvm <- data.frame(Results = c("Accuracy", "95% CI"),
                     Values = c("0.403", "(0.289, 0.525)"))

 
table.svm.res <- kable(ressvm, caption = "Class Weighted Linear Kernel SVM Prediction Results", booktabs=T)
kable_styling(table.svm.res, bootstrap_options = "striped", full_width = F, latex_options = "HOLD_position")
```

```{r}

svmTab <- data.frame(Truth = c("Class 1", "Class 2", "Class 3", "Class 4", "Class 5"), 
                     Class1.Prediction = c(6,1,0,3,0), 
                     Class2.Prediction = c(1,7,1,3,1),
                     class3.Prediction = c(6,1,11,9,1),
                     class4.Prediction = c(2,1,1,3,1),
                     class5.Prediction = c(5,3,3,0,2))
 
table.svm.cm <- kable(svmTab, caption = "Class Weighted Linear Kernel SVM Confusion Matrix", booktabs=T)
kable_styling(table.svm.cm, bootstrap_options = "striped", full_width = F, latex_options = "HOLD_position")
```

# Conclusion 

# Appendix

## References
[1] A Practical Guide to Support Vector Machines. (2003). National Taiwan University. https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf

[2] One-vs-Rest and One-vs-One for Mutli-Class Classification. (2020). Machine Learning Mastery. https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/

## Models
```{r factor-categoricals}
sesame.q1 <- sesame

sesame.q1$site <- as.factor(sesame.q1$site)
sesame.q1$sex <- as.factor(sesame.q1$sex)
sesame.q1$viewcat <- as.factor(sesame.q1$viewcat)
sesame.q1$setting <- as.factor(sesame.q1$setting)
sesame.q1$viewenc <- as.factor(sesame.q1$viewenc)
```

```{r}
# Scaling Variables
sesame.q1$bodyDiff <- rescale(sesame.q1$bodyDiff, to = c(0, 30)) 
sesame.q1$letDiff <- rescale(sesame.q1$letDiff, to = c(0, 30)) 
sesame.q1$formDiff <- rescale(sesame.q1$formDiff, to = c(0, 30))
sesame.q1$numbDiff <- rescale(sesame.q1$numbDiff, to = c(0, 30))
sesame.q1$relatDiff <- rescale(sesame.q1$relatDiff, to = c(0, 30))
sesame.q1$clasfDiff <- rescale(sesame.q1$clasfDiff, to = c(0, 30))
```

```{r}
# Test-Train Split
set.seed(1)
train <- sample(1:nrow(sesame.q1), nrow(sesame.q1)*0.7)

training = sesame.q1[train,]
testing =sesame.q1[-train, ]
```

```{r}
lin.mod1.full <- lm(bodyDiff ~ (site + sex + age + viewcat + setting + viewenc), data = training)
summary(lin.mod1.full)
AIC(lin.mod1.full)

yhat <- predict(lin.mod1.full, newdata = testing)
y.test <- testing[, "bodyDiff"]

# Test MSE
mean((yhat-y.test)^2)
```

```{r}
# Split data between x and y
x.train <- model.matrix(bodyDiff~site + sex + age + viewcat + setting + viewenc, training)[,-1]
y.train <- training$bodyDiff

x.test <- model.matrix(bodyDiff~site + sex + age + viewcat + setting + viewenc, testing)[,-1]
y.test <- testing$bodyDiff

# set seed
set.seed(1)

# cross validation for lambda
cv.out <- cv.glmnet(x.train, y.train, alpha = 0) # setting alpha = 0 indicates ridge regression

# optimal lambda value
best.lam <- cv.out$lambda.min

# ridge regression model with optimal lambda
ridge.mod1.full <- glmnet(x.train, y.train, alpha = 0, lambda = best.lam)

# calculate predictions
ridge.pred <- predict(ridge.mod1.full, s = best.lam, newx = x.test)

# MSE calculation
mean((ridge.pred - y.test)^2)
```

```{r}
lin.mod2.full <- lm(letDiff ~ (site + sex + age + viewcat + setting + viewenc), data = training)
summary(lin.mod2.full)
AIC(lin.mod2.full)

yhat <- predict(lin.mod2.full, newdata = testing)
y.test <- testing[, "letDiff"]

# Test MSE
mean((yhat-y.test)^2)
```

```{r}
# Split data between x and y
x.train <- model.matrix(letDiff~site + sex + age + viewcat + setting + viewenc, training)[,-1]
y.train <- training$letDiff

x.test <- model.matrix(letDiff~site + sex + age + viewcat + setting + viewenc, testing)[,-1]
y.test <- testing$letDiff

# set seed
set.seed(1)

# cross validation for lambda
cv.out <- cv.glmnet(x.train, y.train, alpha = 0) # setting alpha = 0 indicates ridge regression

# optimal lambda value
best.lam <- cv.out$lambda.min

# ridge regression model with optimal lambda
ridge.mod2.full <- glmnet(x.train, y.train, alpha = 0, lambda = best.lam)

# calculate predictions
ridge.pred <- predict(ridge.mod2.full, s = best.lam, newx = x.test)

# MSE calculation
mean((ridge.pred - y.test)^2)
```

```{r}
lin.mod3.full <- lm(formDiff ~ (site + sex + age + viewcat + setting + viewenc), data = training)
summary(lin.mod3.full)
AIC(lin.mod3.full)

yhat <- predict(lin.mod3.full, newdata = testing)
y.test <- testing[, "formDiff"]
# Test MSE
mean((yhat-y.test)^2)
```

```{r}
# Split data between x and y
x.train <- model.matrix(formDiff~site + sex + age + viewcat + setting + viewenc, training)[,-1]
y.train <- training$formDiff

x.test <- model.matrix(formDiff~site + sex + age + viewcat + setting + viewenc, testing)[,-1]
y.test <- testing$formDiff

# set seed
set.seed(1)

# cross validation for lambda
cv.out <- cv.glmnet(x.train, y.train, alpha = 0) # setting alpha = 0 indicates ridge regression

# optimal lambda value
best.lam <- cv.out$lambda.min

# ridge regression model with optimal lambda
ridge.mod3.full <- glmnet(x.train, y.train, alpha = 0, lambda = best.lam)

# calculate predictions
ridge.pred <- predict(ridge.mod3.full, s = best.lam, newx = x.test)

# MSE calculation
mean((ridge.pred - y.test)^2)
```

```{r}
lin.mod4.full <- lm(numbDiff ~ (site + sex + age + viewcat + setting + viewenc), data = training)
summary(lin.mod4.full)
AIC(lin.mod4.full)

yhat <- predict(lin.mod4.full, newdata = testing)
y.test <- testing[, "numbDiff"]

# Test MSE
mean((yhat-y.test)^2)
```

```{r}
# Split data between x and y
x.train <- model.matrix(numbDiff~site + sex + age + viewcat + setting + viewenc, training)[,-1]
y.train <- training$numbDiff

x.test <- model.matrix(numbDiff~site + sex + age + viewcat + setting + viewenc, testing)[,-1]
y.test <- testing$numbDiff

# set seed
set.seed(1)

# cross validation for lambda
cv.out <- cv.glmnet(x.train, y.train, alpha = 0) # setting alpha = 0 indicates ridge regression

# optimal lambda value
best.lam <- cv.out$lambda.min

# ridge regression model with optimal lambda
ridge.mod4.full <- glmnet(x.train, y.train, alpha = 0, lambda = best.lam)

# calculate predictions
ridge.pred <- predict(ridge.mod4.full, s = best.lam, newx = x.test)

# MSE calculation
mean((ridge.pred - y.test)^2)
```

```{r}
lin.mod5.full <- lm(relatDiff ~ (site + sex + age + viewcat + setting + viewenc), data = training)
summary(lin.mod5.full)
AIC(lin.mod5.full)

yhat <- predict(lin.mod5.full, newdata = testing)
y.test <- testing[, "relatDiff"]

# Test MSE
mean((yhat-y.test)^2)
```

```{r}
# Split data between x and y
x.train <- model.matrix(relatDiff~site + sex + age + viewcat + setting + viewenc, training)[,-1]
y.train <- training$relatDiff

x.test <- model.matrix(relatDiff~site + sex + age + viewcat + setting + viewenc, testing)[,-1]
y.test <- testing$relatDiff

# set seed
set.seed(1)

# cross validation for lambda
cv.out <- cv.glmnet(x.train, y.train, alpha = 0) # setting alpha = 0 indicates ridge regression

# optimal lambda value
best.lam <- cv.out$lambda.min

# ridge regression model with optimal lambda
ridge.mod5.full <- glmnet(x.train, y.train, alpha = 0, lambda = best.lam)

# calculate predictions
ridge.pred <- predict(ridge.mod5.full, s = best.lam, newx = x.test)

# MSE calculation
mean((ridge.pred - y.test)^2)
```

```{r}
lin.mod6.full <- lm(clasfDiff ~ (site + sex + age + viewcat + setting + viewenc), data = training)
summary(lin.mod6.full)
AIC(lin.mod6.full)

yhat <- predict(lin.mod6.full, newdata = testing)
y.test <- testing[, "clasfDiff"]

# Test MSE
mean((yhat-y.test)^2)
```

```{r}
# Split data between x and y
x.train <- model.matrix(clasfDiff~site + sex + age + viewcat + setting + viewenc, training)[,-1]
y.train <- training$clasfDiff

x.test <- model.matrix(clasfDiff~site + sex + age + viewcat + setting + viewenc, testing)[,-1]
y.test <- testing$clasfDiff

# set seed
set.seed(1)

# cross validation for lambda
cv.out <- cv.glmnet(x.train, y.train, alpha = 0) # setting alpha = 0 indicates ridge regression

# optimal lambda value
best.lam <- cv.out$lambda.min

# ridge regression model with optimal lambda
ridge.mod6.full <- glmnet(x.train, y.train, alpha = 0, lambda = best.lam)

# calculate predictions
ridge.pred <- predict(ridge.mod6.full, s = best.lam, newx = x.test)

# MSE calculation
mean((ridge.pred - y.test)^2)
```

## Regression Tree Models 

### Model 1

```{r}
set.seed(1)

reg.tree.1 <- tree(bodyDiff ~ site + sex + age + viewcat + setting + viewenc, sesame.q1, subset = train)
summary(reg.tree.1)

cv.reg.tree.1  <- cv.tree(reg.tree.1)
plot(cv.reg.tree.1$size, cv.reg.tree.1$dev, type = "b")

```

```{r}

prune.reg.tree.1  <- prune.tree(reg.tree.1, best = 4)
plot(prune.reg.tree.1)
text(prune.reg.tree.1, pretty = 0)

yhat <- predict(prune.reg.tree.1, newdata = testing)
y.test <- testing[, "bodyDiff"]

# Test MSE
mean((yhat-y.test)^2)

```

## Model 2

```{r}

set.seed(1)

reg.tree.2 <- tree(letDiff ~ site + sex + age + viewcat + setting + viewenc, sesame.q1, subset = train)
summary(reg.tree.2)

cv.reg.tree.2  <- cv.tree(reg.tree.2)
plot(cv.reg.tree.2$size, cv.reg.tree.2$dev, type = "b")

```

```{r}

prune.reg.tree.2  <- prune.tree(reg.tree.2, best = 3)
plot(prune.reg.tree.2)
text(prune.reg.tree.2, pretty = 0)

yhat <- predict(prune.reg.tree.2, newdata = testing)
y.test <- testing[, "letDiff"]

# Test MSE
mean((yhat-y.test)^2)

```

## Model 3

```{r}

set.seed(1)

reg.tree.3 <- tree(formDiff ~ site + sex + age + viewcat + setting + viewenc, sesame.q1, subset = train)
summary(reg.tree.3)

cv.reg.tree.3  <- cv.tree(reg.tree.3)
plot(cv.reg.tree.3$size, cv.reg.tree.3$dev, type = "b")

```

```{r}

prune.reg.tree.3  <- prune.tree(reg.tree.3, best = 2)
plot(prune.reg.tree.3)
text(prune.reg.tree.3, pretty = 0)

yhat <- predict(prune.reg.tree.3, newdata = testing)
y.test <- testing[, "formDiff"]

# Test MSE
mean((yhat-y.test)^2)

```

## Model 4

```{r}

set.seed(1)

reg.tree.4 <- tree(numbDiff ~ site + sex + age + viewcat + setting + viewenc, sesame.q1, subset = train)
summary(reg.tree.4)

cv.reg.tree.4  <- cv.tree(reg.tree.4)
plot(cv.reg.tree.4$size, cv.reg.tree.4$dev, type = "b")

```

```{r}

prune.reg.tree.4  <- prune.tree(reg.tree.4, best = 3)
plot(prune.reg.tree.4)
text(prune.reg.tree.4, pretty = 0)

yhat <- predict(prune.reg.tree.4, newdata = testing)
y.test <- testing[, "numbDiff"]

# Test MSE
mean((yhat-y.test)^2)

```

## Model 5

```{r}

set.seed(1)

reg.tree.5 <- tree(relatDiff ~ site + sex + age + viewcat + setting + viewenc, sesame.q1, subset = train)
summary(reg.tree.5)

cv.reg.tree.5  <- cv.tree(reg.tree.5)
plot(cv.reg.tree.5$size, cv.reg.tree.5$dev, type = "b")

```

```{r}

prune.reg.tree.5  <- prune.tree(reg.tree.5, best = 5)
plot(prune.reg.tree.5)
text(prune.reg.tree.5, pretty = 0)

yhat <- predict(prune.reg.tree.5, newdata = testing)
y.test <- testing[, "relatDiff"]

# Test MSE
mean((yhat-y.test)^2)

```

## Model 6

```{r}

set.seed(1)

reg.tree.6 <- tree(clasfDiff ~ site + sex + age + viewcat + setting + viewenc, sesame.q1, subset = train)
summary(reg.tree.6)

cv.reg.tree.6  <- cv.tree(reg.tree.6)
plot(cv.reg.tree.6$size, cv.reg.tree.6$dev, type = "b")

```

```{r}

prune.reg.tree.6  <- prune.tree(reg.tree.6, best = 2)
plot(prune.reg.tree.6)
text(prune.reg.tree.6, pretty = 0)

yhat <- predict(prune.reg.tree.6, newdata = testing)
y.test <- testing[, "clasfDiff"]

# Test MSE
mean((yhat-y.test)^2)

```


## Q.2 Classification Question: Can we use the pre-test scores and other demographic variables to predict which region the children came from?

### Extra SVM models

```{r svm-fitting-without-classWeight, eval= F}
set.seed(315)
costs <- c(0.001, 0.01, 0.1, 1, 5, 10, 100)
# c(0.1, 0.2, 0.5, 0.7, 1, 2, 3, 4)
gammas <- seq(0, 4, by=0.1)
degrees <- c(1,2,3,4,5)

linear.tune <- tune(svm, site~female+ male + sd_age+sd_pBod+sd_plet+sd_pform + sd_pnumb+sd_prelat+sd_pclasf+sd_peabody, 
                    data=train.data, kernel="linear",
                    ranges=list(cost=costs))

radial.tune <- tune(svm, site~female + male + sd_age+sd_pBod+sd_plet+sd_pform + sd_pnumb+sd_prelat+sd_pclasf+sd_peabody, 
                    data=train.data, kernel="radial",
                    ranges=list(cost=costs, 
                                gamma=gammas))

sigmoid.tune <- tune(svm, site~female + male + sd_age+sd_pBod+sd_plet+sd_pform + sd_pnumb+sd_prelat+sd_pclasf+sd_peabody, 
                    data=train.data, kernel="sigmoid",
                    ranges=list(cost=costs, 
                                gamma=gammas))

poly.tune <- tune(svm, site~female + male + sd_age+sd_pBod+sd_plet+sd_pform + sd_pnumb+sd_prelat+sd_pclasf+sd_peabody,
                  data=train.data, kernel="polynomial",
                  ranges=list(cost=costs,
                              degree=degrees))
```

```{r svm-confmatrix, eval=F}
linear.cm <- table(true=test.data[, "site"],
                          pred=predict(linear.tune$best.model, newdata=test.data))

radial.cm <- table(true=test.data[, "site"],
                          pred=predict(radial.tune$best.model, newdata=test.data))

sigmoid.cm <- table(true=test.data[,"site"], 
                    pred=predict(sigmoid.tune$best.model, newdata=test.data))

poly.cm <- table(true=test.data[, "site"],
                 pred=predict(poly.tune$best.model, newdata=test.data))
```

```{r, eval = F}
confusionMatrix(linear.cm)

confusionMatrix(radial.cm)

confusionMatrix(sigmoid.cm)

confusionMatrix(poly.cm)
```

```{r, eval=F}
radial.weighted <- tune(svm, site~female + male + sd_age+sd_pBod+sd_plet+sd_pform + sd_pnumb+sd_prelat+sd_pclasf+sd_peabody, 
                    data=train.data, kernel="radial",
                    ranges=list(cost=costs, 
                                gamma=gammas),
                    class.weights=c("1"=weight.1,
                                    "2"=weight.2,
                                    "3"=weight.3,
                                    "4"=weight.4,
                                    "5"=weight.5),
                    class.type="one.versus.one")
#radial.tune <- tune(svm, site~sex+age+prebody+prelet+preform+prenumb+prerelat+preclasf, 
#                    data=train.data, kernel="radial",
#                    ranges=list(cost=costs, 
#                                gamma=gammas))

sigmoid.weighted <- tune(svm, site~female + male + sd_age+sd_pBod+sd_plet+sd_pform + sd_pnumb+sd_prelat+sd_pclasf+sd_peabody, 
                    data=train.data, kernel="sigmoid",
                    ranges=list(cost=costs, 
                                gamma=gammas),
                    class.weights=c("1"=weight.1,
                                    "2"=weight.2,
                                    "3"=weight.3,
                                    "4"=weight.4,
                                    "5"=weight.5),
                    class.type="one.versus.one")

poly.weighted <- tune(svm, site~female + male + sd_age+sd_pBod+sd_plet+sd_pform + sd_pnumb+sd_prelat+sd_pclasf+sd_peabody, 
                    data=train.data, kernel="sigmoid",
                    ranges=list(cost=costs, 
                                degree=degrees),
                    class.weights=c("1"=weight.1,
                                    "2"=weight.2,
                                    "3"=weight.3,
                                    "4"=weight.4,
                                    "5"=weight.5),
                    class.type="one.versus.one")

radial.w.cm <- table(true=test.data[, "site"],
                          pred=predict(radial.weighted$best.model, newdata=test.data))

sigmoid.w.cm <- table(true=test.data[,"site"], 
                    pred=predict(sigmoid.weighted$best.model, newdata=test.data))

poly.w.cm <- table(true=test.data[, "site"],
                 pred=predict(poly.weighted$best.model, newdata=test.data))
```

```{r, eval=F}
confusionMatrix(radial.w.cm)

confusionMatrix(sigmoid.w.cm)

confusionMatrix(poly.w.cm)
```

