---
title: "Analyzing and Predicting the Relationships between Sesame Street Viewership and Test Scores among School Children"
author: "Angela Wang, QiHan Zhou, Matthew Murray, Michelle Mao, Sara Lemus, Sophie Dalldorf"
date: "12/13/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=F, message=F)
knitr::opts_chunk$set(fig.pos = "h", out.extra = "")
```

```{r loading-libraries}
library(foreign)
library(tidyverse)
library(nnet)
library(e1071)
library(tree)
library(gbm)
library(randomForest)
library(caret)
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidyverse)
library(patchwork)
library(UBL)
library(scales)
library(glmnet)
library(kableExtra)
library(car)
library(gtsummary)
library(flextable)
library(formattable)
library(lmvar)
sesame <- read.dta("sesame.dta")
sesame <- sesame %>%
  mutate(site=factor(site)) %>%
  mutate(bodyDiff = postbody - prebody,
         letDiff = postlet - prelet,
         formDiff = postform - preform,
         numbDiff = postnumb - prenumb,
         relatDiff = postrelat - prerelat,
         clasfDiff = postclasf - preclasf)
sesame.sd <- sesame%>%
  mutate(sd_pBod = scale(prebody, center = TRUE, scale = TRUE),
         sd_plet = scale(prelet, center = TRUE, scale = TRUE),
         sd_pform = scale(preform, center = TRUE, scale = TRUE),
         sd_pnumb = scale(prenumb, center = TRUE, scale = TRUE),
         sd_prelat = scale(prerelat, center = TRUE, scale = TRUE),
         sd_pclasf = scale(preclasf, center = TRUE, scale = TRUE),
         sd_peabody = scale(peabody, center = TRUE, scale = TRUE), 
         sd_age = scale(age, center =TRUE, scale = TRUE),
         male=if_else(sex==1, 1, 0),
         female=if_else(sex==2, 1, 0))
```


# Introduction

Sesame Street is a hallmark feature of American television that has been educating young children since 1969. The publicly-funded show was created with the goal of supplementing the learning of low-income preschoolers who did not have the access to early education that their other peers did. However, Sesame Street has had far-reaching success across all segments of the American population, regardless of background. In fact, according to one 1996 survey,  95% of all American children had watched Sesame Street by the time they turned 3. A more recent 2018 estimate suggested that about 86 million Americans had watched the show as young children [1]. Such an influential TV series that has affected the childhoods of countless Americans warrants further investigation into the educational impacts of the program itself. 

This project in particular makes use of a dataset that was compiled by researchers in the early 1970s meant to assess whether or not Sesame Street was achieving its goals of educating economically disadvantaged children. The researchers tested a variety of children from different backgrounds on critical skills like numbers and letters. They then re-tested these children after they had watched Sesame Street. Through our research, we seek to answer two questions related to this dataset about childhood education:

1. Can we predict the change between pre-test and post-test scores after children watch Sesame Street and analyze the most influential predictor variables?
2. Can we use information about the pre-test score and other demographic variables to predict what type of background the child is from?

There are a variety of situational and demographic factors that were collected by researchers when examining the relationship between watching Sesame Street and a child's performance on skill tests. For our first research question, we seek to not only incorporate some of these covariates to build an accurate predictive model, but also to gain some insight as to which of these covariates are associated with a higher difference between the child's performance on the test. Through this research goal, we ultimately hope to build a useful predictive model that helps us further understand the educational impact of Sesame Street on children of all backgrounds.

The second question investigates the relationship between various subject pre-test scores and children of different income and living situations. By understanding this, we can develop a more more nuanced understanding of how low income children may be affected by their background. The absence of early education for low-income students in America has been a problem for decades, and still persists today. One statistic suggests that a mere 18% of low-income students are enrolled in high-quality pre-K. Location also plays a role: 55% of children who live in rural areas lack access to high-quality education [2]. This lack of access to pre-K disproportionately affects students from disadvantaged backgrounds, as they are left less ready for kindergarten and may suffer further setbacks compared to their peers as they continue their education into elementary, middle, and high school. Thus, it is critical to further understand the relationship between the background of children and how this may affect their ability to test well on critical subjects like learning and math. 

We address our research questions using various machine modeling techniques. More specifically, for our first research question we will perform a comparison of least squares, ridge, and tree regression models to find the model that is most interpretable and accurate, which after performing our analysis ends up being least sqaures regression. For our second question, we will fit a multi-class SVM classifier to predict what background a child comes from.


# Data

The data was collected in the early 1970s by researchers from the Educational Testing Service (ETS), and the actual dataset itself was retrieved from a Columbia University database [3]. The researchers sampled children representative of 5 economically advantaged and disadvantaged populations in the US. The children were randomly assigned to either receive encouragement to watch Sesame Street or not to receive encouragement. The children in the encouragement group were given promotional materials about Sesame Street, and the ETS staff called and visited every week to ensure children were actively watching Sesame Street. Those who were not in the encouragement group did not receive any of this. The children were intially tested in 1970 on skills relating to body parts, letters, forms, numbers, relational terms, and classfication skills, and the post test evaluations were one year later after watching the show. While we considered using other data sources, we decided that it was not feasible due to the fact that there is not any further information available about the participants in the study and their backgrounds (other than the dataset from the Columbia University database).

Our data contains 34 variables. The ID refers to a subject's identification number. `site` refers to the age and background information of the child. A site value of 1 indicates a 3-5 year old disadvantaged child from the inner city. A site value of 2 represents a 4 year old advantaged child from the suburbs. A value of 3 represents an advantaged rural child. A site value of 4 indicates a disadvantaged rural child. Lastly, a value of 5 represents a disadvantaged Spanish speaking child. For the sex, a value of 1 indicates male, and a value of 2 indicates female. The age category is the child's age in months. The `viewcat` variable represents  the frequency of viewing Sesame Street (1 = rarely, 2 = once/twice per week, 3 = 3-5 times a week, 4 = more than 5 times per week). The `setting` is where Sesame Street was viewed; a value of 1 indicates home and a value of 2 indicates school. The `viewenc` column refers to if the child was encouraged to watch or not (1 = child not encouraged, 2 = child encouraged). 

The `prebody, prelet, preform, prenumb, prerelat,` and `preclasf` columns all describe pretest scores on varying types of assessments (body parts, letters, forms, numbers, relational terms, and classification skills, respectively). The columns labeled `postbody, postlet, postform, postnumb, postrelat,` and `postclasf` are the children's respective post-test scores. We created the following variables - `bodydiff, letDiff, formDiff, numbDiff, relatDiff, clasfDiff` - to represent the difference in post-test scores and pretest scores for each child. Lastly, `peabody` represents a score of "mental age" for vocabulary maturity from the Peabody Picture Vocabulary Test.

Our main focus will be on the new variables we created (`bodyDiff, letDiff, formDiff, numbDiff, relatDiff, clasfDiff`) and variables related to how often the children watch Sesame Street (namely, `viewcat` and `regular`). Lastly, we will look into the demographics of the children, including `site, sex,` and `age`.

## Plotting Differences in Unstandardized Scores By Students' Backgrounds

```{r moreEDA}
boxbody <- ggplot(sesame, aes(x = bodyDiff, y = site)) + 
  geom_boxplot(fill = "lightblue") +
  labs(title = "Body Parts", x = "Post - Pre") +
  scale_y_discrete(labels=c("disad city", "adv suburb", "adv rural", "disad rural", "disad Spanish")) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

boxlet <- ggplot(sesame, aes(x = letDiff, y = site)) + 
  geom_boxplot(fill = "lightblue") +
  labs(title = "Letters", x = "Post - Pre") +
  scale_y_discrete(labels=c("disad city", "adv suburb", "adv rural", "disad rural", "disad Spanish")) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

boxform <- ggplot(sesame, aes(x = formDiff, y = site)) + 
  geom_boxplot(fill = "lightblue") +
  labs(title = "Forms", x = "Post - Pre") +
  scale_y_discrete(labels=c("disad city", "adv suburb", "adv rural", "disad+ rural", "disad Spanish")) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

boxnumb <- ggplot(sesame, aes(x = numbDiff, y = site)) + 
  geom_boxplot(fill = "lightblue") +
  labs(title = "Numbers", x = "Post - Pre") +
  scale_y_discrete(labels=c("disad city", "adv suburb", "adv rural", "disad rural", "disad Spanish")) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

boxrelat <- ggplot(sesame, aes(x = relatDiff, y = site)) + 
  geom_boxplot(fill = "lightblue") +
  labs(title = "Relational Terms", x = "Post - Pre") +
  scale_y_discrete(labels=c("disad city", "adv suburb", "adv rural", "disad rural", "disad Spanish")) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

boxclasf <- ggplot(sesame, aes(x = clasfDiff, y = site)) + 
  geom_boxplot(fill = "lightblue") +
  labs(title = "Classification Skills", x = "Post - Pre") +
  scale_y_discrete(labels=c("disad city", "adv suburb", "adv rural", "disad rural", "disad Spanish")) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

(boxbody + boxclasf + boxform) / (boxnumb + boxrelat + boxlet)

```

We decided to plot the distribution of post-test - pre-test scores for each test using boxplots. First, we created separate plots for different values of the variable ``site``, as we conjectured that a child's background would have an effect on his or her change in test scores. From these plots, it appears that children from some backgrounds show greater changes in test scores than others, and we will further examine this relationship in our models. 

We also created separate plots for different values of the variable ``viewcat`` because we also assumed that the frequency at which children watch Sesame Street would affect their respective changes in test scores. One key takeaway from these plots is that accross all tests, the median value of post-test score - pre-test score is greater for children who watch Sesame Street very frequently (greater than 5 times a week) than for children who rarely watch Sesame Street. This trend suggests that Sesame Street has a positive educational impact on children by helping them improve their test scores. Again, however, this takeaway is something that we will examine further with our models. 

## Plotting Differences in Unstandardized Scores By Viewership Frequency 

```{r}

boxbody <- ggplot(sesame, aes(x = bodyDiff, y = as.factor(viewcat))) + 
  geom_boxplot(fill = "lightpink") + 
  labs(title = "Body Parts", x = "Post - Pre") +
  scale_y_discrete(labels=c("rarely", "1-2 times a week", "3-5 times a week", ">5 times a week")) +   
  theme_minimal() +
  theme(axis.title.y = element_blank())

boxlet <- ggplot(sesame, aes(x = letDiff, y = as.factor(viewcat))) + 
  geom_boxplot(fill = "lightpink") +
  labs(title = "Letters", x = "Post - Pre") +
  scale_y_discrete(labels=c("rarely", "1-2 times a week", "3-5 times a week", ">5 times a week")) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

boxform <- ggplot(sesame, aes(x = formDiff, y = as.factor(viewcat))) + 
  geom_boxplot(fill = "lightpink") +
  labs(title = "Forms", x = "Post - Pre") +
  scale_y_discrete(labels=c("rarely", "1-2 times a week", "3-5 times a week", ">5 times a week")) + 
  theme_minimal() +
  theme(axis.title.y = element_blank())

boxnumb <- ggplot(sesame, aes(x = numbDiff, y = as.factor(viewcat))) + 
  geom_boxplot(fill = "lightpink") +
  labs(title = "Numbers", x = "Post - Pre") +
  scale_y_discrete(labels=c("rarely", "1-2 times a week", "3-5 times a week", ">5 times a week")) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

boxrelat <- ggplot(sesame, aes(x = relatDiff, y = as.factor(viewcat))) + 
  geom_boxplot(fill = "lightpink") +
  labs(title = "Relational Terms", x = "Post - Pre") +
  scale_y_discrete(labels=c("rarely", "1-2 times a week", "3-5 times a week", ">5 times a week")) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

boxclasf <- ggplot(sesame, aes(x = clasfDiff, y = as.factor(viewcat))) + 
  geom_boxplot(fill = "lightpink") +
  labs(title = "Classification Skills", x = "Post - Pre") +
  scale_y_discrete(labels=c("rarely", "1-2 times a week", "3-5 times a week", ">5 times a week")) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

(boxbody + boxclasf + boxform) / (boxnumb + boxrelat + boxlet)

```

```{r eval =F}
ggplot(sesame, aes(x = numbDiff, y = as.factor(viewenc))) + 
  geom_boxplot(fill = "lightblue") +
  labs(title = "NumbDiff by Viewenc", x = "Post - Pre on Numbers") 
ggplot(sesame, aes(x = letDiff, y = as.factor(viewenc))) + 
  geom_boxplot(fill = "lightblue") +
  labs(title = "LetDiff by Viewenc", x = "Post - Pre on Numbers") 
ggplot(sesame, aes(x = bodyDiff, y = as.factor(viewenc))) + 
  geom_boxplot(fill = "lightblue") +
  labs(title = "BodyDiff by Viewenc", x = "Post - Pre on Numbers") 
ggplot(sesame, aes(x = relatDiff, y = as.factor(viewenc))) + 
  geom_boxplot(fill = "lightblue") +
  labs(title = "RelatDiff by Viewenc", x = "Post - Pre on Numbers") 
ggplot(sesame, aes(x = clasfDiff, y = as.factor(viewenc))) + 
  geom_boxplot(fill = "lightblue") +
  labs(title = "ClasfDiff by Viewenc", x = "Post - Pre on Numbers") 
ggplot(sesame, aes(x = letDiff, y = as.factor(viewenc))) + 
  geom_boxplot(fill = "lightblue") +
  labs(title = "LetDiff by Viewenc", x = "Post - Pre on Numbers") 
```

# Methodology 

## Research Question 1

For our first research question, we seek to predict the difference in pre and post-test scores after children watch Sesame Street as well as identify the most influential covariates. More specifically, for all 6 tests we choose to let the difference in pre and post-test scores for that given test be the response variable, and look at the following variables as covariates: ``site``, ``sex``, `age`, `viewcat`, ``setting``, ``viewenc``. We choose to look at site (which is representative of the child's background) because, as mentioned in the introduction, Sesame Street is particularly aimed at helping underprivileged young children learn fundamental skills. We want to further study this potential association between the background of the child and the difference between their two test scores. We adjust for the age and sex of the child as well as whether or not they viewed the show at home or school, since all of these variables may potentially have an effect on their pre and post-test scores. We also include the frequency of when children watched Sesame Street (`viewcat`) as well as whether or not they were encouraged to watch Sesame Street by the researchers. This is motivated by EDA plots that suggest that for all tests, children who were encouraged to watch have higher test differences than those who were not, and children who watched Sesame Street frequently had higher median test differences than children who watched rarely. Thus, we wanted to include these factors in our model since we believe they are important to include when trying to accurately model the difference between pre and post-test scores.

Before including these covariates in any model, we first encoded all of them as type categorical which is appropriate given that each number represents a level rather than a numerical value.
 

```{r eval = F}
ggplot(sesame, aes(x = numbDiff, y = as.factor(viewcat))) + 
  geom_boxplot(fill = "lightpink") +
  labs(title = "NumbDiff by ViewCat", x = "Post - Pre on Numbers") 
ggplot(sesame, aes(x = letDiff, y = as.factor(viewcat))) + 
  geom_boxplot(fill = "lightpink") +
  labs(title = "LetDiff by ViewCat", x = "Post - Pre on Numbers") 
ggplot(sesame, aes(x = bodyDiff, y = as.factor(viewcat))) + 
  geom_boxplot(fill = "lightpink") +
  labs(title = "BodyDiff by ViewCat", x = "Post - Pre on Numbers") 
ggplot(sesame, aes(x = relatDiff, y = as.factor(viewcat))) + 
  geom_boxplot(fill = "lightpink") +
  labs(title = "RelatDiff by ViewCat", x = "Post - Pre on Numbers") 
ggplot(sesame, aes(x = clasfDiff, y = as.factor(viewcat))) + 
  geom_boxplot(fill = "lightpink") +
  labs(title = "ClasfDiff by ViewCat", x = "Post - Pre on Numbers") 
ggplot(sesame, aes(x = letDiff, y = as.factor(viewcat))) + 
  geom_boxplot(fill = "lightpink") +
  labs(title = "LetDiff by ViewCat", x = "Post - Pre on Numbers") 
```


```{r eval = FALSE}
ggplot(sesame, aes(x = numbDiff, y = as.factor(viewenc))) + 
  geom_boxplot(fill = "lightblue") +
  labs(title = "NumbDiff by Viewenc", x = "Post - Pre on Numbers") 
ggplot(sesame, aes(x = letDiff, y = as.factor(viewenc))) + 
  geom_boxplot(fill = "lightblue") +
  labs(title = "LetDiff by Viewenc", x = "Post - Pre on Numbers") 
ggplot(sesame, aes(x = bodyDiff, y = as.factor(viewenc))) + 
  geom_boxplot(fill = "lightblue") +
  labs(title = "BodyDiff by Viewenc", x = "Post - Pre on Numbers") 
ggplot(sesame, aes(x = relatDiff, y = as.factor(viewenc))) + 
  geom_boxplot(fill = "lightblue") +
  labs(title = "RelatDiff by Viewenc", x = "Post - Pre on Numbers") 
ggplot(sesame, aes(x = clasfDiff, y = as.factor(viewenc))) + 
  geom_boxplot(fill = "lightblue") +
  labs(title = "ClasfDiff by Viewenc", x = "Post - Pre on Numbers") 
ggplot(sesame, aes(x = letDiff, y = as.factor(viewenc))) + 
  geom_boxplot(fill = "lightblue") +
  labs(title = "LetDiff by Viewenc", x = "Post - Pre on Numbers") 
```



After identifying the response variables and covariates, we had to choose an appropriate model. There are many different ways to predict our continuous response variable; however we identified 3 potential models: least squares regression, ridge regression, and regression trees. All 3 of these models allow us to predict the test score differences, though have different ways of doing so and different degrees of interpretability. In order to pick a final model that best aligns with our research question, we chose to fit these 3 different models on our 6 different test score differences. We will then compare the predictive performance of each model as well as respective model properties to make an informed decision about which model is best to use for our results and conclusion. Next, we will discuss how we specifically fit all 3 model types to the data.


#### Least-squares Regression
We first decided to consider least-squares regression models as a potential model due to the fact that they provide apt inference into the relationship between the covariates and the response variable. Our regression models follow the general form:

$$ \text{Post-test}-\text{Pre-test} = \beta_0 \ + \ \beta_1 \ (\text{Site = 2})+ \ \beta_2\ (\text{Site = 3})+ \ \beta_3 \ (\text{Site = 4})+ \ \beta_5 \ (\text{Site = 5}) + \ \beta_6 \ (\text{Sex = Female}) \\$$ 
$$+ \ \beta_7 \ (age) \ 
+ \ \beta_8 \ (\text{ViewCat = Once or Twice per Week})+ \ \beta_9\ (\text{ViewCat = Three to Five Times per Week})+ \\$$
$$\beta_{10} \ (\text{ViewCat = 5 Times a Week or More on Avg})
+  \ \beta_{11} \ (\text{Setting = School}) +  \ \beta_{12} \ (\text{ViewEnc = Not Encouraged})$$

We perform this regression for all 6 different test types. 

#### Ridge Regression

We also decided on selecting ridge regression as a candidate model due to the fact that ridge regression often provides performance improvements by shrinking slope coefficients. The shrinkage is achieved by applying a shrinkage parameter, $\lambda$, to the Euclidean norm of a slope coefficient. We tuned our $\lambda$ parameter using 10-fold cross validation. More specifically, we computed the cross-validation error rate for our model for a grid of $\lambda$ values. Thereafter, we selected the $\lambda$ value for which the cross-validation error was the smallest. 

Initially, our least-squares and ridge regression models also took into account all possible two-way interaction terms between the variables. However, we noticed that both the Akaike Information Criterion (AIC) and adjusted-$R^2$ values were higher for models that did not include any interaction effects. That reason, coupled with the lack of apparent interaction effects in our exploratory data analysis and the fact that we wanted our linear models to be interpretable, is why we decided not to include any interaction effects in our final linear models. 

#### Regression Trees

Our third and final candidate model is the regression tree. Similar to linear regression, regression trees are not the most competitive in terms of predictive power or accuracy, but they are easy to interpret and can provide important inferences into the relationships between covariates and the response variable. 

The process of building the trees was very simple. After initially building the tree, we then performed cross validation to determine if pruning the tree would improve the deviance (the sum of the squared errors) for the tree. Using the cross validation plot that was then created, we choose the number of nodes for the tree that minimized the deviance. 

We also noticed that each of the the tests are scored on different scales. For example, the scores for the test on knowledge of body parts (noted by ``bodyDiff``) ranged from 0-32, while those of the test on letters (noted by ``letDiff``) ranged from 0-58. To be able to aptly compare the mean squared error (MSE) between models, we also decided to convert each response variable to the same range. More specifically, we scaled each variable to the arbitrary range [0, 30]. Lastly, we randomly split the data between testing and training, using 70% of the data for training and 30% of the data for testing.

#### Selecting the Final Model

```{r}
df1 <- data.frame(Response = c("Body Parts Diff", "Letters Diff", "Forms Diff", "Numbers Diff", "Relational Terms Diff", "Classification Skills Diff"), 
Least.Squares.Test.MSE = c(24.01, 14.31, 13.26, 15.51, 18.94, 48.40), Ridge.Test.MSE = c(21.61, 14.20, 12.83, 14.63, 19.86, 44.26), Regression.Tree.Test.MSE = c(20.60, 15.41, 14.92, 15.91, 19.89, 45.53))

table <- kable(df1, caption = "Test MSEs for 3 Candidate Models", booktabs=T)
kable_styling(table, bootstrap_options = "striped", full_width = T, latex_options = "HOLD_position")
```

The above table compares the predictive performance of all model types. We saw that all 3 candidate models have fairly comparable performance on our test data set, with ridge regression having the lowest MSE for 4 out of the 6 tests. Least squares regression was the second best for predictive performance, scoring the lowest MSE for the relational terms test difference and having similar MSEs as ridge regression for most tests. Regression trees subsequently performed the worst.

Our goal for this question was not only to build a predictive model, but also to identify the variables that were influential in understanding the difference between a child's pre and post-test score. Thus, we also took into consideration the intepretability of all 3 model types, since we wanted to choose a model which we could draw useful inferences from. While ridge regression gave the best predictive performance, it was less intepretable than linear regression due to its shrunken coefficients. While trees were the  most interpretable model, they performed the worst compared to the other two across all tests. Thus, we chose to pick least squares regression as our final model type, since it best allowed us to make sufficient predictions and inferences regarding our response variable, difference in test scores.  

After selecting least squares regression, we performed model diagnostics. All 6 models seemed to aptly satisfy the linearity condition, as the residual plots for each of the linear regression models have no discernible pattern or structure. The satisfaction of this condition ensured that there was in fact a linear relationship between the response variable and the predictors. The constant variance (homoscedasticity) condition of linear regression seems to have been satisfied as well, as the vertical spread of the residuals was relatively constant across each of the plots. Lastly, the normality assumption of linear regression seemed to be aptly satisfied. For each of the models, the points fell along a straight diagonal line on the normal quantile (QQ) plot, indicating that the residuals followed a normal distribution. 


## Research Question 2

In order to address our second research question, predicting whether a child came from an disadvantaged background or not based on their pretest scores and demographic information, we utilized a support vector machine (SVM). As mentioned in the introduction, disadvantaged children often have worse access to educational opportunities and consequently suffer worse academic outcomes as well. Thus, we were interested in analyzing the strength of this association and seeing if we could predict the socioeconomic background of a child using their pre-test scores. We also wanted to adjust for other demographic factors such as age and sex, so we included these variables in our model as well. We expect younger children to have lower scores on their pre-test, and we also wanted to adjust for any possible effects of gender. Our model uses the sex, age, and all pre-test score variables to predict our response variable, site. Since our response variable is a categorical variable, a SVM is a valid choice to answer our research question. SVMs also have an advantage of using various kernels to place complex decision boundaries and capture relationships well. We also tried implementing a classification tree and a logistic model to answer this question (See Appendix for more details). The three models had relatively similar performance in terms of overall prediction accuracy. However, we decided to present the SVM model as our final model since we spent a considerable amount of time researching different optimization methods to boost its performance and felt like it would best accomplish our goal of prediction. Our full model formula is:


$$SVM(\text{Site} \mathtt{\sim} \text{Female} + \text{Male} + \text{Age} + \text{Body Parts}_{pretest}+ \text{Letters}_{pretest} + $$
$$\text{Forms}_{pretest} + \text{Numbers}_{pretest} + \text{Relational Terms}_{pretest} + \text{Classification Skills}_{pretest} )$$

We split 70% of our data into a training set and 30% into a testing set and analyzed the performance of our model on the test set. Since we are interested in high predictive power, we implemented a variety of different methods to improve our SVM models' performance. Standardizing the predictor variables in SVM and encoding categorical variables has been shown to improve performance for SVMs [4]. Thus, we used the standardized forms of the continuous variables in our model and encoded the female and male variables. We did indeed observe a small improvement in prediction accuracy across all models. However, one problem that particularly piqued our interests is that we were making no predictions for classes 4 or 5 for site. This could be due to sites 4 & 5 having a smaller number of observations than the other classes. We first tried using the Synthetic Minority Over-Sampling Technique (SMOTE) to increase observations within these classes; however, this technique had negligible effects on our accuracy. We then tried weighting the classes and used the formula below to assign weights to each class and specify "one versus one" comparison, which has been suggested to yield better prediction than "one versus all" [5].

$$
w_j =\frac{n}{kn_j} , \text{ n is total number of data points, k is number of classes, }n_j \text{ is the number of data in class j}
$$

We tested our model using linear, radial, sigmoid, and polynomial kernels and compared the predictive accuracy between these models. After the class weight assignment, the SVM models began to make prediction on classes 4 and 5. However, doing so came at the cost of overall accuracy. Thus, more of the other observations were being misclassified, but the few observations of sites 4 and 5 were correctly classified. To remedy this issue, we began to experiment with the class weights and increase the weights of sites 4 and 5 by roughly 0.5 until we reached the highest predictive power. The model with the highest accuracy was the linear kernel SVM with weights 0.8 on site 1, 0.87 on site 2, 0.75 on site 3, 1.5 on site 4, and 3 on site 5 and a cross validation selected cost parameter of 0.1. Since this model has the highest predictive accuracy of all others that we had tried, we settled on this model as our final model. There are no explicit tests for SVM model diagnostics; however, there seems to be no major issues with our model fit.

# Results

## Research Question 1

As mentioned in our methodology section, we choose to model the relationship between the difference in test scores and our selected covariates using least squares regression. The below models are abbreviated and only include coefficients that were statistically significant at the $\alpha = 0.05$ level. Full model output can be found in the Appendix.

$$ \textbf{Body Parts Post Score - Pre Score} = 16.43 \ + \ 2.45 \ (\text{Site = 3}) \ + \ 2.97 \ (\text{Site = 4}) \ + \ 3.30 \ (\text{Site= 5}) $$
$$ \textbf{Letters Post Score - Pre Score} = 10.52 \ + \ 3.09 \ (\text{Site = 2}) \ - \ 2.86 \ (\text{Site = 3}) \ + 4.85 \ (\text{ViewCat = 3 to 5 Times Per Week}) +$$
$$ 4.77 \ (\text{ViewCat = 5 Times a Week or More on Avg}) $$
$$ \textbf{Forms Post Score - Pre Score} = 16.04 \ + \ 2.48 \ (\text{ViewCat = 5 Times a Week or More on Avg})$$
$$ \textbf{Numbers Post Score - Pre Score} = 15.39 \ + \ 2.56 \ (\text{Site = 2}) \ + \ 3.20 \ (\text{ViewCat = 3 to 5 Times Per Week}) + $$
$$ 3.64 \ (\text{ViewCat = 5 Times a Week or More on Avg})$$
$$ \textbf{Relational Terms Post Score - Pre Score} = 18.49 \ + \ 2.42 \ (\text{Site = 4}) \ + \ 2.96 \ +$$ $$(\text{ViewCat = 5 Times a Week or More on Avg}) $$
$$ \textbf{Classification Skills Post Score - Pre Score} = 10.95 \ + \ 4.06 \ (\text{ViewCat = 5 Times a Week or More on Avg}) $$

### Interpretations  

Looking at the least-squares  models, we can note a few important features regarding predicting whether a childâ€™s test scores will increase. For ``bodydiff``, or the difference between post and pre-tests scores for the recognition of body parts, we see that the intercept is 16.43. This large positive intercept may be explained by the fact that all 6 tests have more positive values for difference than negative values (meaning that more children had higher post test scores compared to their pre test scores). For all following variable interpretations, we assume that each change is the expected difference in test score difference holding all else constant. We also see that for all the levels of site that were statistically significant, our model estimated a positive coefficient value. Specifically, we see that if a child is ``site3``, or an advantaged rural child, the difference between their scores goes up by 2.45. For a disadvantaged rural child, the difference increases even more by 2.97 and for a disadvantaged Spanish speaking child, the difference goes up by 3.30. This indicates that for children from more disadvantaged backgrounds, the difference between the scores increases as compared to the two other sites.

For differences in letter recognition scores, we note the intercept is 10.52, the baseline difference. We see for 4 year old advantaged child from the suburbs (``site2``) this difference increases by 3.09. However, interestingly enough for an advantaged rural child (``site3``), the difference in test scores decreases by 2.86. It also seems that the frequency of watching Sesame Street has an effect on letter scores with `viewcat3` or watching 3-5 times a week increasing the difference by 4.85 and watching more than 5 times a week increasing the difference by 4.77. 

For recognizing forms, the intercept is 16.04 and it seems that watching Sesame Street more than 5 times a week (``viewcat4``) , further increases this difference by 2.48. For numbers, the intercept is 15.39. We note that  for an 4 year old advantaged child from the suburbs, the difference in number scores increases by 2.56. Watching Sesame Street 3-5 times a week (``viewcat3``) increases the difference by 3.20, and watching more than 5 times a week increases the difference by 3.64.  For difference in relational term scores, the intercept is 18.49, the largest of all intercepts and the difference is further increased for a disadvantaged Spanish speaking child by 2.42. For those who watch more than 5 times a week, the difference increases by 2.96. For classification score differences, the intercept is 10.95. The only variable that increases this difference is the frequency of viewing, with those that watch Sesame Street more than 5 times a week increasing the difference by 4.06.


## Research Question 2

As evidenced in the tables below, the linear kernel SVM has a higher accuracy than the other kernels we also tried. The class weighted linear kernel SVM has a prediction accuracy of 0.403 on our test data with a confidence interval of 0.289 to 0.525. From the confusion matrix, we can also see that our model predicts well for classes 2 and 3 and struggles slightly with classes 1,4, and 5. Specifically, our model tends to predict class 3 often when the true class is 4, and it tends to predict classes 3 and 5 as commonly as class 1 when the true class is 1.

```{r classMod-accuracy-tab}
classfMod.tab <- data.frame(Model = c("Linear Kernel SVM",
                                      "Radial Kernel SVM",
                                      "Sigmoid Kernel SVM",
                                      "Polynomial Kernel SVM"),
                            Accuracy = c(0.4028, 0.3611, 
                                         0.2639, 0.2917))
all.accuracy <- kable(classfMod.tab, caption = "Classification Model Accuracy Table", booktabs=T)
kable_styling(all.accuracy, bootstrap_options = "striped", full_width = F, latex_options = "HOLD_position")
```

```{r test-train-split}
set.seed(3241)

n <- nrow(sesame)
train.index <- sample(1:n, size = floor(0.7*n), replace=FALSE)
train.data <- sesame.sd[train.index,]
test.data <- sesame.sd[-train.index,]
```

```{r classWeight-assigning}
set.seed(315)
costs <- c(0.001, 0.01, 0.1, 1, 5, 10, 100)
total.weight <- 60+55+64+43+18
weight.1 <- total.weight/(5*60)
weight.2 <- total.weight/(5*55)
weight.3 <- total.weight/(5*64)  
weight.4 <- total.weight/(5*43)  
weight.5 <- total.weight/(5*18)  

#increase the weight of class 4 & 5 by a little bit over 0.4(chosen arbitraily)
weight.4 <- 1.5
weight.5 <- 3

linear.weighted <- tune(svm, site~female+ male + sd_age+sd_pBod+sd_plet+sd_pform + sd_pnumb+sd_prelat+sd_pclasf+sd_peabody, 
                    data=train.data, kernel="linear",
                    ranges=list(cost=costs),
                    class.weights=c("1"=weight.1,
                                    "2"=weight.2,
                                    "3"=weight.3,
                                    "4"=weight.4,
                                    "5"=weight.5),
                    class.type="one.versus.one")

linear.w.cm <- table(true=test.data[, "site"],
                          pred=predict(linear.weighted$best.model, newdata=test.data))
```

```{r, include = F}
confusionMatrix(linear.w.cm)
```

```{r}
ressvm <- data.frame(Results = c("Accuracy", "95% CI"),
                     Values = c("0.403", "(0.289, 0.525)"))

 
table.svm.res <- kable(ressvm, caption = "Class Weighted Linear Kernel SVM Prediction Results", booktabs=T)
kable_styling(table.svm.res, bootstrap_options = "striped", full_width = F, latex_options = "HOLD_position")
```

```{r}

svmTab <- data.frame(Truth = c("Class 1", "Class 2", "Class 3", "Class 4", "Class 5"), 
                     Class1.Prediction = c(6,1,0,3,0), 
                     Class2.Prediction = c(1,7,1,3,1),
                     Class3.Prediction = c(6,1,11,9,1),
                     Class4.Prediction = c(2,1,1,3,1),
                     Class5.Prediction = c(5,3,3,0,2))
 
table.svm.cm <- kable(svmTab, caption = "Class Weighted Linear Kernel SVM Confusion Matrix", booktabs=T)
kable_styling(table.svm.cm, bootstrap_options = "striped", full_width = F, latex_options = "HOLD_position")
```

# Conclusion 

## Research Question 1

The findings from our least squares regression models is that the frequency at which a child watches Sesame Street (``viewcat``) and a child's background (``site``) are the most important variables in predicting the difference in test scores for a child. In 5 of the 6 least squares regression models, at least one level of ``viewcat`` is a significant predictor for the response variable. We also saw that at least one level of site was significant in 4 out of the 6 models that we found (body parts, letters, numbers, and relational). This speaks to two important insights. First, the importance of `viewcat` suggests that children who watch Sesame Street more frequently are more likely to see greater differences in test scores compared to children who rarely watch Sesame Street. More specifically, the greatest increase in differences are seen in children who watch Sesame Street at least 3-5 times per week or more than 5 times a week. This relationship suggests that Sesame Street does have an impact on children's learning of fundamental subject matters and that the show accomplishes its goals of affecting learning outcomes. The second important insight from our findings is that, at least to some extent, the background of a child is related to their difference between pre and post-test scores. This implies that the background of a child is important when thinking about how children learn and perform on these tests.

We also cared about how well our model could accurately predict the difference in pre and post-test scores. Being able to predict these scores accurately is an indicator that we appropriately selected our covariates and were able to fit a model that alllows us to understand the relationship between these variables and the differences in test scores. When comparing performance across all 6 tests, we find that our models had the best predictive performance on the letters and forms tests, as these two had the lowest test MSEs. Our model had the worst performance on the classification skills test score difference, with an MSE value of 48. While we do feel like we satisfied our research goal of being able to build a predictive model, there is undoubtedly room for improvement when it comes to the predictive performance of our models, especially for the classification skills and body parts test. 

One of the main limitations to our conclusions from our least squares models has to do with the relationship between our data and time. While our model does find statistically significant associations between covariates like a child's background and how often the child watches Sesame Street, we suspect that time may be a variable that is affecting the difference in between post-test and pre-test scores. Children can learn and develop a lot in a year, especially at such a young age (all children were between 3 and 5 years old). Thus, it is possible that time is a latent variable explaining some of this difference in testing scores. This hypothesis is supported by the fact that all of our least squares models had R-squared values below 0.4, suggesting that our models do not explain a lot of the variation in our response and there may be additional data that is affecting our response that was not included in our model specification. 


## Research Question 2
Our goal for this research question was to build a model that could accurately predict the background that a child came from based on their age, sex, and pretest scores. Even though our model is not extremely accurate, we are still able to predict with 40% accuracy which background a child comes from using their pre-test scores and age and sex. This indicates that there is some relationship between a family's income level and living area and the performance of their children on critical subjects like letters and numbers. While our model does not perform very poorly on our data, there is still room for improvement. Of all the models we tried, the best accuracies were all right around 40%. This could be a flaw of the data as we have only 240 observations which is not very many. Additionally, there is some imbalance in our data set as we have less observations of children from sites 4 and 5, which were disadvantaged rural children and disadvantaged Spanish speaking children respectively. This may be a flaw of the data collection, and our data may not be representative of the general population. Additionally, there may have existed a clustering structure that caused the SVM model to struggle with separating the different levels of site. This phenomenon may be illustrated with the confusion matrix having some difficulty separating between certain classes. When the true site is 3, indicating an advantaged child from a rural area, the model tends to predict site 4 as well which represents an disadvantaged rural child. The differences between children from these backgrounds may be quite small, so the model does not distinguish them quite well. This may be due to the fact that children living in rural areas have less access to education in general as mentioned in the Introduction. Therefore, socioeconomic differences between children living in these areas may not matter as much since high quality education is difficult to access for both high and low income families. We may be able to improve our model and gain greater predictive accuracy with more children in our data set and more data. Including more variables into our data set, such as child attendance in preschool or daycare, may also be useful for prediction and answering this research question. 

Another possible inference from these results is that there are more factors that explain pretest scores. For example, parent income level or highest degree of parent education may be a more useful response variable than an arbitrary determination of advantaged or disadvantaged. We do not have information on what the researchers determined to be advantaged or disadvantaged, so their definitions of these categories may be different than our opinions or those of the general public. Our data was also collected in the 70s which is a vastly different time period than the one we are currently in, so categories of advantaged and disadvantaged may be different now compared to back then. A more concrete response variable like the ones previously mentioned might provide more useful insights. Future work may be focused on collecting information on these socioeconomic variables and performing our analysis once again to see if our results differ. There are also other optimization methods that we haven't explored; for example, we can try other feature transformations to make the data more separable or try to design a customized SVM kernel that is better tailored to the distribution of our data.

## General Takeaways
For both research questions, we found that the site variable, which represents the socioeconomic background of a child, plays an important role on pre-test scores as well as the difference between pre and post-test scores. This finding is consistent with our original beliefs as there is a well-known connection between worse educational opportunities and educational performance for low-income and rural children. Our research supports this conclusion, indicating that more work is necessary to ensure that children of disadvantaged, low-income, and rural communities are able to access high-quality and affordable education from a young age to ensure proper learning of essential topics such as letters and numbers and ensure future academic success. 

\newpage

# Appendix

## References
[1] Sesame Street. (2021). Wikipedia. https://en.wikipedia.org/wiki/Sesame_Street

[2] The Need. (2021). First Five Years Fund. https://www.ffyf.org/why-it-matters/the-need/

[3] Index of /~gelman/arm/examples/sesame. (n.d.). Retrieved December 12, 2021, from http://www.stat.columbia.edu/~gelman/arm/examples/sesame/

[3] A Practical Guide to Support Vector Machines. (2003). National Taiwan University. https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf

[4] One-vs-Rest and One-vs-One for Multi-Class Classification. (2020). Machine Learning Mastery. https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/

## Research Question 1: Linear Models
```{r factor-categoricals}
sesame.q1 <- sesame

sesame.q1$site <- as.factor(sesame.q1$site)
sesame.q1$sex <- as.factor(sesame.q1$sex)
sesame.q1$viewcat <- as.factor(sesame.q1$viewcat)
sesame.q1$setting <- as.factor(sesame.q1$setting)
sesame.q1$viewenc <- as.factor(sesame.q1$viewenc)
```

```{r}
# Scaling Variables
sesame.q1$bodyDiff <- rescale(sesame.q1$bodyDiff, to = c(0, 30)) 
sesame.q1$letDiff <- rescale(sesame.q1$letDiff, to = c(0, 30)) 
sesame.q1$formDiff <- rescale(sesame.q1$formDiff, to = c(0, 30))
sesame.q1$numbDiff <- rescale(sesame.q1$numbDiff, to = c(0, 30))
sesame.q1$relatDiff <- rescale(sesame.q1$relatDiff, to = c(0, 30))
sesame.q1$clasfDiff <- rescale(sesame.q1$clasfDiff, to = c(0, 30))
```

Below are the R squared values for each test.

```{r}
df2 <- data.frame(Response = c("Body Parts Diff", "Letters Diff", "Forms Diff", "Numbers Diff", "Relational Terms Diff", "Classification Skills Diff"), 
R.Squared = c(0.159, 0.37, 0.063, 0.139, 0.107, 0.094))

table <- kable(df2, caption = "Multiple R-Squared Values for Least-Squares Regression Models", booktabs=T)
kable_styling(table, bootstrap_options = "striped", full_width = F, latex_options = "HOLD_position")
```

```{r}
# Test-Train Split
set.seed(1)
train <- sample(1:nrow(sesame.q1), nrow(sesame.q1)*0.7)

training = sesame.q1[train,]
testing =sesame.q1[-train, ]
```

### Least Squares Model for Difference in Body Part Test Scores and Diagnostics


```{r}
lin.mod1.full <- lm(bodyDiff ~ (site + sex + age + viewcat + setting + viewenc), data = training, y = TRUE)
#summary(lin.mod1.full)
#AIC(lin.mod1.full)
tbl_regression(lin.mod1.full, 
               pvalue_fun = ~style_pvalue(.x, digits = 3),
               label = list(site ~ "Site", sex ~ "Sex", age ~ "Age", viewenc~ "Encouraged to Watch", setting ~ "Where Sesame Street is Viewed", viewcat ~ "Frequency of Watching"),
               intercept=T,  estimate_fun = function(x) style_sigfig(x, digits = 3)) %>%
  bold_p(t = 0.05) %>%
  bold_labels() %>%
  italicize_levels() %>%
  as_flex_table() 

yhat <- predict(lin.mod1.full, newdata = testing)
y.test <- testing[, "bodyDiff"]

# Test MSE
#mean((yhat-y.test)^2)
```


```{r}
# residual plot
par(mfrow = c(2,2))
res <- resid(lin.mod1.full)
plot(fitted(lin.mod1.full), res, main = "Residual Plot")
abline(0,0)

# QQ plot
plot_qq(lin.mod1.full)
```

### Ridge Regression Model for Difference in Body Part Test Scores 
```{r}
# Split data between x and y
x.train <- model.matrix(bodyDiff~site + sex + age + viewcat + setting + viewenc, training)[,-1]
y.train <- training$bodyDiff

x.test <- model.matrix(bodyDiff~site + sex + age + viewcat + setting + viewenc, testing)[,-1]
y.test <- testing$bodyDiff

# set seed
set.seed(1)

# cross validation for lambda
cv.out <- cv.glmnet(x.train, y.train, alpha = 0) # setting alpha = 0 indicates ridge regression

# optimal lambda value
best.lam <- cv.out$lambda.min

# ridge regression model with optimal lambda
ridge.mod1.full <- glmnet(x.train, y.train, alpha = 0, lambda = best.lam)

# calculate predictions
ridge.pred <- predict(ridge.mod1.full, s = best.lam, newx = x.test)
kable(predict(ridge.mod1.full, type = 'coefficients', s = best.lam)[1:12,], col.names = c("Estimate"), digits = 3)

# MSE calculation
#mean((ridge.pred - y.test)^2)
```

### Least Squares Model for Difference in Letters Test Scores and Diagnostics
```{r}
lin.mod2.full <- lm(letDiff ~ (site + sex + age + viewcat + setting + viewenc), data = training, y = TRUE)
#summary(lin.mod2.full)
#AIC(lin.mod2.full)

tbl_regression(lin.mod2.full, 
               pvalue_fun = ~style_pvalue(.x, digits = 3),
               label = list(site ~ "Site", sex ~ "Sex", age ~ "Age", viewenc~ "Encouraged to Watch", setting ~ "Where Sesame Street is Viewed", viewcat ~ "Frequency of Watching"),
               intercept=T,  estimate_fun = function(x) style_sigfig(x, digits = 3)) %>%
  bold_p(t = 0.05) %>%
  bold_labels() %>%
  italicize_levels() %>%
  as_flex_table() 

yhat <- predict(lin.mod2.full, newdata = testing)
y.test <- testing[, "letDiff"]

# Test MSE
#mean((yhat-y.test)^2)
```

```{r}
par(mfrow=c(2,2))
# residual plot
res <- resid(lin.mod2.full)
plot(fitted(lin.mod2.full), res, main = "Residual Plot")
abline(0,0)

# QQ plot
plot_qq(lin.mod2.full)
```

### Ridge Regression Model for Difference in Letters Test Scores
```{r}
# Split data between x and y
x.train <- model.matrix(letDiff~site + sex + age + viewcat + setting + viewenc, training)[,-1]
y.train <- training$letDiff

x.test <- model.matrix(letDiff~site + sex + age + viewcat + setting + viewenc, testing)[,-1]
y.test <- testing$letDiff

# set seed
set.seed(1)

# cross validation for lambda
cv.out <- cv.glmnet(x.train, y.train, alpha = 0) # setting alpha = 0 indicates ridge regression

# optimal lambda value
best.lam <- cv.out$lambda.min

# ridge regression model with optimal lambda
ridge.mod2.full <- glmnet(x.train, y.train, alpha = 0, lambda = best.lam)

# calculate predictions
ridge.pred <- predict(ridge.mod2.full, s = best.lam, newx = x.test)
kable(predict(ridge.mod2.full, type = 'coefficients', s = best.lam)[1:12,], col.names = c("Estimate"), digits = 3, "latex", booktabs = T) %>% 
  kable_styling(position = "center")

# MSE calculation
#mean((ridge.pred - y.test)^2)
```

### Least Squares Model for Difference in Forms Test Scores and Diagnostics
```{r}
lin.mod3.full <- lm(formDiff ~ (site + sex + age + viewcat + setting + viewenc), data = training, y = TRUE)
#summary(lin.mod3.full)
#AIC(lin.mod3.full)

tbl_regression(lin.mod3.full, 
               pvalue_fun = ~style_pvalue(.x, digits = 3),
               label = list(site ~ "Site", sex ~ "Sex", age ~ "Age", viewenc~ "Encouraged to Watch", setting ~ "Where Sesame Street is Viewed", viewcat ~ "Frequency of Watching"),
               intercept=T,  estimate_fun = function(x) style_sigfig(x, digits = 3)) %>%
  bold_p(t = 0.05) %>%
  bold_labels() %>%
  italicize_levels() %>%
  as_flex_table() 

yhat <- predict(lin.mod3.full, newdata = testing)
y.test <- testing[, "formDiff"]
# Test MSE
#mean((yhat-y.test)^2)
```

```{r}
par(mfrow=c(2,2))
# residual plot
res <- resid(lin.mod3.full)
plot(fitted(lin.mod3.full), res, main = "Residual Plot")
abline(0,0)

# QQ plot
plot_qq(lin.mod3.full)
```

### Ridge Regression Model for Difference in Forms Test Scores
```{r}
# Split data between x and y
x.train <- model.matrix(formDiff~site + sex + age + viewcat + setting + viewenc, training)[,-1]
y.train <- training$formDiff

x.test <- model.matrix(formDiff~site + sex + age + viewcat + setting + viewenc, testing)[,-1]
y.test <- testing$formDiff

# set seed
set.seed(1)

# cross validation for lambda
cv.out <- cv.glmnet(x.train, y.train, alpha = 0) # setting alpha = 0 indicates ridge regression

# optimal lambda value
best.lam <- cv.out$lambda.min

# ridge regression model with optimal lambda
ridge.mod3.full <- glmnet(x.train, y.train, alpha = 0, lambda = best.lam)

# calculate predictions
ridge.pred <- predict(ridge.mod3.full, s = best.lam, newx = x.test)
kable(predict(ridge.mod3.full, type = 'coefficients', s = best.lam)[1:12,], col.names = c("Estimate"), digits = 3)

# MSE calculation
#mean((ridge.pred - y.test)^2)
```

### Least Squares Model for Difference in Numbers Test Scores and Diagnostics
```{r}
lin.mod4.full <- lm(numbDiff ~ (site + sex + age + viewcat + setting + viewenc), data = training, y = TRUE)
#summary(lin.mod4.full)
#AIC(lin.mod4.full)

tbl_regression(lin.mod4.full, 
               pvalue_fun = ~style_pvalue(.x, digits = 3),
               label = list(site ~ "Site", sex ~ "Sex", age ~ "Age", viewenc~ "Encouraged to Watch", setting ~ "Where Sesame Street is Viewed", viewcat ~ "Frequency of Watching"),
               intercept=T,  estimate_fun = function(x) style_sigfig(x, digits = 3)) %>%
  bold_p(t = 0.05) %>%
  bold_labels() %>%
  italicize_levels() %>%
  as_flex_table() 

yhat <- predict(lin.mod4.full, newdata = testing)
y.test <- testing[, "numbDiff"]

# Test MSE
#mean((yhat-y.test)^2)
```

```{r}
par(mfrow=c(2,2))
# residual plot
res <- resid(lin.mod4.full)
plot(fitted(lin.mod4.full), res, main = "Residual Plot")
abline(0,0)

# QQ plot
plot_qq(lin.mod4.full)

```

### Ridge Regression Model for Difference in Numbers Test Scores 
```{r}
# Split data between x and y
x.train <- model.matrix(numbDiff~site + sex + age + viewcat + setting + viewenc, training)[,-1]
y.train <- training$numbDiff

x.test <- model.matrix(numbDiff~site + sex + age + viewcat + setting + viewenc, testing)[,-1]
y.test <- testing$numbDiff

# set seed
set.seed(1)

# cross validation for lambda
cv.out <- cv.glmnet(x.train, y.train, alpha = 0) # setting alpha = 0 indicates ridge regression

# optimal lambda value
best.lam <- cv.out$lambda.min

# ridge regression model with optimal lambda
ridge.mod4.full <- glmnet(x.train, y.train, alpha = 0, lambda = best.lam)

# calculate predictions
ridge.pred <- predict(ridge.mod4.full, s = best.lam, newx = x.test)
kable(predict(ridge.mod4.full, type = 'coefficients', s = best.lam)[1:12,], col.names = c("Estimate"), digits = 3)

# MSE calculation
#mean((ridge.pred - y.test)^2)
```

### Least Squares Model for Difference in Relational Terms Test Scores and Diagnostics
```{r}
lin.mod5.full <- lm(relatDiff ~ (site + sex + age + viewcat + setting + viewenc), data = training, y = TRUE)
#summary(lin.mod5.full)
#AIC(lin.mod5.full)

tbl_regression(lin.mod5.full, 
               pvalue_fun = ~style_pvalue(.x, digits = 3),
               label = list(site ~ "Site", sex ~ "Sex", age ~ "Age", viewenc~ "Encouraged to Watch", setting ~ "Where Sesame Street is Viewed", viewcat ~ "Frequency of Watching"),
               intercept=T,  estimate_fun = function(x) style_sigfig(x, digits = 3)) %>%
  bold_p(t = 0.05) %>%
  bold_labels() %>%
  italicize_levels() %>%
  as_flex_table() 

yhat <- predict(lin.mod5.full, newdata = testing)
y.test <- testing[, "relatDiff"]

# Test MSE
#mean((yhat-y.test)^2)
```

```{r}
par(mfrow=c(2,2))
# residual plot
res <- resid(lin.mod5.full)
plot(fitted(lin.mod5.full), res, main = "Residual Plot")
abline(0,0)

# QQ plot
plot_qq(lin.mod5.full)

```

### Ridge Regression Model for Difference in Relational Terms Test Scores
```{r}
# Split data between x and y
x.train <- model.matrix(relatDiff~site + sex + age + viewcat + setting + viewenc, training)[,-1]
y.train <- training$relatDiff

x.test <- model.matrix(relatDiff~site + sex + age + viewcat + setting + viewenc, testing)[,-1]
y.test <- testing$relatDiff

# set seed
set.seed(1)

# cross validation for lambda
cv.out <- cv.glmnet(x.train, y.train, alpha = 0) # setting alpha = 0 indicates ridge regression

# optimal lambda value
best.lam <- cv.out$lambda.min

# ridge regression model with optimal lambda
ridge.mod5.full <- glmnet(x.train, y.train, alpha = 0, lambda = best.lam)

# calculate predictions
ridge.pred <- predict(ridge.mod5.full, s = best.lam, newx = x.test)
kable(predict(ridge.mod5.full, type = 'coefficients', s = best.lam)[1:12,], col.names = c("Estimate"), digits = 3)

# MSE calculation
#mean((ridge.pred - y.test)^2)
```

### Least Squares Model for Difference in Classification Skills Test Scores and Diagnostics
```{r}
lin.mod6.full <- lm(clasfDiff ~ (site + sex + age + viewcat + setting + viewenc), data = training, y = TRUE)
#summary(lin.mod6.full)
#AIC(lin.mod6.full)

tbl_regression(lin.mod6.full, 
               pvalue_fun = ~style_pvalue(.x, digits = 3),
               label = list(site ~ "Site", sex ~ "Sex", age ~ "Age", viewenc~ "Encouraged to Watch", setting ~ "Where Sesame Street is Viewed", viewcat ~ "Frequency of Watching"),
               intercept=T,  estimate_fun = function(x) style_sigfig(x, digits = 3)) %>%
  bold_p(t = 0.05) %>%
  bold_labels() %>%
  italicize_levels() %>%
  as_flex_table() 

yhat <- predict(lin.mod6.full, newdata = testing)
y.test <- testing[, "clasfDiff"]

# Test MSE
#mean((yhat-y.test)^2)
```

```{r}
par(mfrow=c(2,2))
# residual plot
res <- resid(lin.mod6.full)
plot(fitted(lin.mod6.full), res, main = "Residual Plot")
abline(0,0)

# QQ plot
plot_qq(lin.mod6.full)
```

### Ridge Regression Model for Difference in Classification Skills Test Scores
```{r}
# Split data between x and y
x.train <- model.matrix(clasfDiff~site + sex + age + viewcat + setting + viewenc, training)[,-1]
y.train <- training$clasfDiff

x.test <- model.matrix(clasfDiff~site + sex + age + viewcat + setting + viewenc, testing)[,-1]
y.test <- testing$clasfDiff

# set seed
set.seed(1)

# cross validation for lambda
cv.out <- cv.glmnet(x.train, y.train, alpha = 0) # setting alpha = 0 indicates ridge regression

# optimal lambda value
best.lam <- cv.out$lambda.min

# ridge regression model with optimal lambda
ridge.mod6.full <- glmnet(x.train, y.train, alpha = 0, lambda = best.lam)

# calculate predictions
ridge.pred <- predict(ridge.mod6.full, s = best.lam, newx = x.test)
kable(predict(ridge.mod6.full, type = 'coefficients', s = best.lam)[1:12,], col.names = c("Estimate"), digits = 3)

# MSE calculation
#mean((ridge.pred - y.test)^2)
```

## Research Question 1: Regression Tree Models 

### Model for Difference in Body Parts Test Scores

```{r, include = F}
library(broom)
set.seed(1)

reg.tree.1 <- tree(bodyDiff ~ site + sex + age + viewcat + setting + viewenc, sesame.q1, subset = train)
summary(reg.tree.1)

cv.reg.tree.1  <- cv.tree(reg.tree.1)
#plot(cv.reg.tree.1$size, cv.reg.tree.1$dev, type = "b")
```
Variables used in tree construction:`site`, `age`, `viewcat`, `sex`, `viewenc`. Terminal nodes: 13. Residual mean deviance: 14.16.

```{r fig.height = 3, fig.width = 4}

prune.reg.tree.1  <- prune.tree(reg.tree.1, best = 4)
plot(prune.reg.tree.1)
text(prune.reg.tree.1, pretty = 0)

yhat <- predict(prune.reg.tree.1, newdata = testing)
y.test <- testing[, "bodyDiff"]

# Test MSE
#mean((yhat-y.test)^2)
```

### Model for Difference in Letters Test Scores

```{r include = F}
set.seed(1)

reg.tree.2 <- tree(letDiff ~ site + sex + age + viewcat + setting + viewenc, sesame.q1, subset = train)
summary(reg.tree.2)

cv.reg.tree.2  <- cv.tree(reg.tree.2)
#plot(cv.reg.tree.2$size, cv.reg.tree.2$dev, type = "b")
```

Variables used in tree construction:`site`, `age`, `viewcat`, `setting`, `viewenc`. Terminal nodes: 12. Residual mean deviance: 18.63.

```{r fig.height = 3, fig.width = 4}

prune.reg.tree.2  <- prune.tree(reg.tree.2, best = 3)
plot(prune.reg.tree.2)
text(prune.reg.tree.2, pretty = 0)

yhat <- predict(prune.reg.tree.2, newdata = testing)
y.test <- testing[, "letDiff"]

# Test MSE
#mean((yhat-y.test)^2)

```

### Model for Difference in Forms Test Scores

```{r include = F}
set.seed(1)

reg.tree.3 <- tree(formDiff ~ site + sex + age + viewcat + setting + viewenc, sesame.q1, subset = train)
summary(reg.tree.3)

cv.reg.tree.3  <- cv.tree(reg.tree.3)
#plot(cv.reg.tree.3$size, cv.reg.tree.3$dev, type = "b")
```

Variables used in tree construction:`site`, `age`, `viewcat`, `setting`. Terminal nodes: 10. Residual mean deviance: 15.88.

```{r fig.height = 3, fig.width = 4}

prune.reg.tree.3  <- prune.tree(reg.tree.3, best = 2)
plot(prune.reg.tree.3)
text(prune.reg.tree.3, pretty = 0)

yhat <- predict(prune.reg.tree.3, newdata = testing)
y.test <- testing[, "formDiff"]

# Test MSE
#mean((yhat-y.test)^2)

```

### Model for Difference in Numbers Test Scores

```{r include = F}
set.seed(1)

reg.tree.4 <- tree(numbDiff ~ site + sex + age + viewcat + setting + viewenc, sesame.q1, subset = train)
summary(reg.tree.4)

cv.reg.tree.4  <- cv.tree(reg.tree.4)
#plot(cv.reg.tree.4$size, cv.reg.tree.4$dev, type = "b")
```

Variables used in tree construction:`site`, `age`,`sex`, `viewcat`, `setting`, `viewenc`. Terminal nodes: 18. Residual mean deviance: 13.64.

```{r fig.height = 3, fig.width = 4}

prune.reg.tree.4  <- prune.tree(reg.tree.4, best = 3)
plot(prune.reg.tree.4)
text(prune.reg.tree.4, pretty = 0)

yhat <- predict(prune.reg.tree.4, newdata = testing)
y.test <- testing[, "numbDiff"]

# Test MSE
#mean((yhat-y.test)^2)

```

### Model for Difference in Relational Terms Test Scores

```{r include = F}
set.seed(1)

reg.tree.5 <- tree(relatDiff ~ site + sex + age + viewcat + setting + viewenc, sesame.q1, subset = train)
summary(reg.tree.5)

cv.reg.tree.5  <- cv.tree(reg.tree.5)
#plot(cv.reg.tree.5$size, cv.reg.tree.5$dev, type = "b")
```

Variables used in tree construction:`site`, `age`, `viewcat`, `setting`, `sex`. Terminal nodes: 10. Residual mean deviance: 15.3.

```{r fig.height = 3, fig.width = 4}

prune.reg.tree.5  <- prune.tree(reg.tree.5, best = 5)
plot(prune.reg.tree.5)
text(prune.reg.tree.5, pretty = 0)

yhat <- predict(prune.reg.tree.5, newdata = testing)
y.test <- testing[, "relatDiff"]

# Test MSE
#mean((yhat-y.test)^2)

```

### Model for Difference in Classification Skills Test Scores
```{r include = F}
set.seed(1)

reg.tree.6 <- tree(clasfDiff ~ site + sex + age + viewcat + setting + viewenc, sesame.q1, subset = train)
summary(reg.tree.6)

cv.reg.tree.6  <- cv.tree(reg.tree.6)
#plot(cv.reg.tree.6$size, cv.reg.tree.6$dev, type = "b")
```

Variables used in tree construction:`site`, `age`, `viewcat`, `sex`. Terminal nodes: 14. Residual mean deviance: 29.66.

```{r fig.height = 3, fig.width = 6}

prune.reg.tree.6  <- prune.tree(reg.tree.6, best = 2)
plot(prune.reg.tree.6)
text(prune.reg.tree.6, pretty = 0)

yhat <- predict(prune.reg.tree.6, newdata = testing)
y.test <- testing[, "clasfDiff"]

# Test MSE
#mean((yhat-y.test)^2)

```


## Research Question 2

```{r svm-fitting-without-classWeight, eval= F}
#additional svm models

set.seed(315)
costs <- c(0.001, 0.01, 0.1, 1, 5, 10, 100)
# c(0.1, 0.2, 0.5, 0.7, 1, 2, 3, 4)
gammas <- seq(0, 4, by=0.1)
degrees <- c(1,2,3,4,5)

linear.tune <- tune(svm, site~female+ male + sd_age+sd_pBod+sd_plet+sd_pform + sd_pnumb+sd_prelat+sd_pclasf+sd_peabody, 
                    data=train.data, kernel="linear",
                    ranges=list(cost=costs))

radial.tune <- tune(svm, site~female + male + sd_age+sd_pBod+sd_plet+sd_pform + sd_pnumb+sd_prelat+sd_pclasf+sd_peabody, 
                    data=train.data, kernel="radial",
                    ranges=list(cost=costs, 
                                gamma=gammas))

sigmoid.tune <- tune(svm, site~female + male + sd_age+sd_pBod+sd_plet+sd_pform + sd_pnumb+sd_prelat+sd_pclasf+sd_peabody, 
                    data=train.data, kernel="sigmoid",
                    ranges=list(cost=costs, 
                                gamma=gammas))

poly.tune <- tune(svm, site~female + male + sd_age+sd_pBod+sd_plet+sd_pform + sd_pnumb+sd_prelat+sd_pclasf+sd_peabody,
                  data=train.data, kernel="polynomial",
                  ranges=list(cost=costs,
                              degree=degrees))
```

```{r svm-confmatrix, eval=F}
linear.cm <- table(true=test.data[, "site"],
                          pred=predict(linear.tune$best.model, newdata=test.data))

radial.cm <- table(true=test.data[, "site"],
                          pred=predict(radial.tune$best.model, newdata=test.data))

sigmoid.cm <- table(true=test.data[,"site"], 
                    pred=predict(sigmoid.tune$best.model, newdata=test.data))

poly.cm <- table(true=test.data[, "site"],
                 pred=predict(poly.tune$best.model, newdata=test.data))
```

```{r, eval = F}
confusionMatrix(linear.cm)

confusionMatrix(radial.cm)

confusionMatrix(sigmoid.cm)

confusionMatrix(poly.cm)
```

```{r, eval=F}
radial.weighted <- tune(svm, site~female + male + sd_age+sd_pBod+sd_plet+sd_pform + sd_pnumb+sd_prelat+sd_pclasf+sd_peabody, 
                    data=train.data, kernel="radial",
                    ranges=list(cost=costs, 
                                gamma=gammas),
                    class.weights=c("1"=weight.1,
                                    "2"=weight.2,
                                    "3"=weight.3,
                                    "4"=weight.4,
                                    "5"=weight.5),
                    class.type="one.versus.one")
#radial.tune <- tune(svm, site~sex+age+prebody+prelet+preform+prenumb+prerelat+preclasf, 
#                    data=train.data, kernel="radial",
#                    ranges=list(cost=costs, 
#                                gamma=gammas))

sigmoid.weighted <- tune(svm, site~female + male + sd_age+sd_pBod+sd_plet+sd_pform + sd_pnumb+sd_prelat+sd_pclasf+sd_peabody, 
                    data=train.data, kernel="sigmoid",
                    ranges=list(cost=costs, 
                                gamma=gammas),
                    class.weights=c("1"=weight.1,
                                    "2"=weight.2,
                                    "3"=weight.3,
                                    "4"=weight.4,
                                    "5"=weight.5),
                    class.type="one.versus.one")

poly.weighted <- tune(svm, site~female + male + sd_age+sd_pBod+sd_plet+sd_pform + sd_pnumb+sd_prelat+sd_pclasf+sd_peabody, 
                    data=train.data, kernel="sigmoid",
                    ranges=list(cost=costs, 
                                degree=degrees),
                    class.weights=c("1"=weight.1,
                                    "2"=weight.2,
                                    "3"=weight.3,
                                    "4"=weight.4,
                                    "5"=weight.5),
                    class.type="one.versus.one")

radial.w.cm <- table(true=test.data[, "site"],
                          pred=predict(radial.weighted$best.model, newdata=test.data))

sigmoid.w.cm <- table(true=test.data[,"site"], 
                    pred=predict(sigmoid.weighted$best.model, newdata=test.data))

poly.w.cm <- table(true=test.data[, "site"],
                 pred=predict(poly.weighted$best.model, newdata=test.data))
```

```{r, eval=F}
confusionMatrix(radial.w.cm)

confusionMatrix(sigmoid.w.cm)

confusionMatrix(poly.w.cm)
```

### Accuracy Table of Random Forest & Logistic Regression

```{r rf-logReg-tab}
rf.log.tab <- data.frame(Model = c("Random Forest",
                                      "Logistic Regression"),
                            Accuracy = c(0.4444, 0.4307))
all.accuracy <- kable(rf.log.tab, caption = "Classification Model Accuracy Table", booktabs=T)
kable_styling(all.accuracy, bootstrap_options = "striped", full_width = F, latex_options = "HOLD_position")
```


### Random Forest
```{r selecting-features-trees}

set.seed(3215)
#tree.data <- sesame %>%
#  select(site, sex, age, viewcat, setting, viewenc, prebody, prelet, preform, 
#         prenumb, prerelat, preclasf)

n <- nrow(sesame)
train.index <- sample(1:n, size = floor(0.7*n), replace=FALSE)
tree.features <- c("site", "age", "viewcat", "setting", "viewenc", "prebody", "prelet","preform", "prenumb", "prerelat", "preclasf")

tree.data <- sesame[, tree.features]
train.tree <- tree.data[train.index,]
test.tree <- tree.data[-train.index,]

```

```{r fitting-randomForest}

rf.tree<- randomForest(site~., data=tree.data, subset=train.index,
                       mtry=4, importance=TRUE)

#importance(rf.tree)

rf.pred <- predict(rf.tree, newdata=test.tree)

tree.conMatrix <- table(true=test.data[,"site"],
                         pred=rf.pred)
confusionMatrix(tree.conMatrix)

#varImpPlot(rf.tree)
```

### Logistic Regression
```{r log-reg, message=FALSE}
                   
tree.features <- c("site", "age", "viewcat", "setting", "viewenc","prebody", "prelet","preform", "prenumb", "prerelat", "preclasf")

tree.log <- sesame[, tree.features]
train.log <- tree.log[train.index,]
test.log <- tree.log[-train.index,]

multinom.log <- multinom(factor(site)~., data=train.log)


summary(multinom.log)

tabs <- table(true=test.log[, "site"],
              pred=predict(multinom.log,newdata=test.log))
confusionMatrix(tabs)

##Backward selection
#iter.msg <- capture.output(log.tune <- train(site~(.)^2, data=train.log, method="multinom", direction="backward",
                  #k=log(3562)))

#tabs2 <- table(true=test.log[, "site"],
              #pred=predict(log.tune,newdata=test.log))
#confusionMatrix(tabs2)
```