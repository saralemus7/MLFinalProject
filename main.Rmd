---
title: "Read Data"
author: ''
date: "11/23/2021"
output:
  pdf_document: default
  html_document: default
---
```{r}
library(foreign)
library(tidyverse)
library(e1071)
library(tree)
library(gbm)
library(randomForest)
library(caret)
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidyverse)
library(patchwork)
library(UBL)
sesame <- read.dta("sesame.dta")
sesame <- sesame %>%
  mutate(site=factor(site)) %>%
  mutate(bodyDiff = postbody - prebody,
         letDiff = postlet - prelet,
         formDiff = postform - preform,
         numbDiff = postnumb - prenumb,
         relatDiff = postrelat - prerelat,
         clasfDiff = postclasf - preclasf)
sesame.sd <- sesame%>%
  mutate(sd_pBod = scale(prebody, center = TRUE, scale = TRUE),
         sd_plet = scale(prelet, center = TRUE, scale = TRUE),
         sd_pform = scale(preform, center = TRUE, scale = TRUE),
         sd_pnumb = scale(prenumb, center = TRUE, scale = TRUE),
         sd_prelat = scale(prerelat, center = TRUE, scale = TRUE),
         sd_pclasf = scale(preclasf, center = TRUE, scale = TRUE),
         sd_peabody = scale(peabody, center = TRUE, scale = TRUE), 
         sd_age = scale(age, center =TRUE, scale = TRUE),
         male=if_else(sex==1, 1, 0),
         female=if_else(sex==2, 1, 0))
```

## Exploratory Data Analysis

```{r viewdata}
head(sesame)
```

### Variables:

The ID refers to a subject's identification number. The site refers to the age and background information of the child. A site value of 1 indicates a 3-5 year old disadvantaged child from the inner city. A site value of 2 represents a 4 year old advantaged child from the suburbs. A value of 3 represents an advantaged rural child. A site value of 4 indicates a disadvantaged rural child. Lastly, a value of 5 represents a disadvantaged Spanish speaking child. For the sex, a value of 1 indicates male, and a value of 2 indicates female. The age category is the child's age in months. The viewcat column is the frequency of viewing Sesame Street (1 = rarely, 2 = once/twice per week, 3 = 3-5 times a week, 4 = more than 5 times per week). The setting is where Sesame Street was viewed; a value of 1 indicates home and a value of 2 indicates school. The viewenc column refers to if the child was encouraged to watch or not (1 = child not encouraged, 2 = child encouraged). Encour is the same variable but with values 0 and 1, respectively. Regular is an indicator variable representing if a child is a regular viewer (0 = rarely watched, 1 = watched once per week or greater). 

The prebody, prelet, preform, prenumb, prerelat, and preclasf columns all decribe pretest scores on varying types of assessments (body parts, letters, forms, numbers, relational terms, and classification skills, respectively). The columns labelled postbody, postlet, postform, postnumb, postrelat, and postclasf are the children's respective posttest scores. Above, we created the following variables - bodydiff, letDiff, formDiff, numbDiff, relatDiff, clasfDiff - to represent the difference in posttest scores and pretest scores for each child. Lastly, peabody represents a score of "mental age" for vocabulary maturity from the Peabody Picture Vocabulary Test.

Our main focus will be on the new variables we created (bodyDiff, letDiff, formDiff, numbDiff, relatDiff, clasfDiff) and variables related to how often the children watch Sesame Street (namely, viewcat and regular). Lastly, we will look into the backgrounds of the children, including site, sex, and age.


### Distributions:

For the purposes of our analysis, we will first look at the distributions of bodyDiff, letDiff, formDiff, numbDiff, relatDiff, and clasfDiff.

```{r visualizetestscoresdiff, message=FALSE}

#want to visualize distributions of bodyDiff, letDiff, formDiff, numbDiff, relatDiff, clasfDiff

bodyDiffplot <- ggplot(sesame, aes(x = bodyDiff)) + 
  geom_histogram(fill = "lightblue") +
  labs(title = "Distribution of bodyDiff", x = "Post - Pre on Body Parts", y = "Count") +
  theme_minimal()

letDiffplot <- ggplot(sesame, aes(x = letDiff)) + 
  geom_histogram(fill = "lightblue") +
  labs(title = "Distribution of letDiff", x = "Post - Pre on Letters", y = "Count") +
  theme_minimal()

formDiffplot <- ggplot(sesame, aes(x = formDiff)) + 
  geom_histogram(fill = "lightblue") +
  labs(title = "Distribution of formDiff", x = "Post - Pre on Forms", y = "Count") +
  theme_minimal()

numbDiffplot <- ggplot(sesame, aes(x = numbDiff)) + 
  geom_histogram(fill = "lightblue") +
  labs(title = "Distribution of numbDiff", x = "Post - Pre on Numbers", y = "Count") +
  theme_minimal()

relatDiffplot <- ggplot(sesame, aes(x = relatDiff)) + 
  geom_histogram(fill = "lightblue") +
  labs(title = "Distribution of relatDiff", x = "Post - Pre Relational Terms", y = "Count") +
  theme_minimal()

clasfDiffplot <- ggplot(sesame, aes(x = clasfDiff)) + 
  geom_histogram(fill = "lightblue") +
  labs(title = "Distribution of clasfDiff", x = "Post - Pre on Classif. Skills", y = "Count") +
  theme_minimal()


bodyDiffplot + letDiffplot + formDiffplot + numbDiffplot + relatDiffplot + clasfDiffplot

```

The six variables above were calculated by subtracting pre-test scores from post-test scores, so they are all numerical. The distributions of these six variables (bodyDiff, letDiff, formDiff, numbDiff, relatDiff, and clasfDiff) all appear to be roughly normal and unimodal. BodyDiff, letDiff, formDiff, relatDiff, and classDiff do not appear to have any obvious extreme outliers. Numbdiff, however, seems to be slightly left-skewed with outliers to the left -20. All of the six variables appear to have centers between 2 and 4. 



We will now examine the distrubtions of the variables related to how often children watch Sesame Street (namely, viewcat and regular).

```{r visualizingwatchingbehavior, message=FALSE}

# want to visualize distributions of viewcat and regular

viewcatplot <- ggplot(sesame, aes(x = factor(viewcat))) +
  geom_bar(fill = "lightblue") +
  labs(title = "Distribution of Viewcat", x = "Frequency of viewing Sesame Street", y = "Count") +
  scale_x_discrete("Frequency of Viewing Sesame Street", labels=c("rarely", "1-2 pw", "3-5 pw", ">5 pw")) +
  theme_minimal() +
  geom_text(aes(label = ..count..), stat = "count", vjust = 1.5, colour = "white")


regularplot <- ggplot(sesame, aes(x = factor(regular))) +
  geom_bar(fill = "lightblue") +
  labs(title = "Distribution of Regular", y = "Count") +
  scale_x_discrete(labels=c("rarely watched", "watched >= 1 per week")) +
  theme_minimal() +
  theme(axis.title.x = element_blank()) +
  geom_text(aes(label = ..count..), stat = "count", vjust = 1.5, colour = "white")

viewcatplot + regularplot

```

Both of these variables are categorical. On the left, viewcat appears to have a roughly uniform distribution, with "rarely" having the least amount of children and 3-5 times per week having the most (the range is only 10 children, so all of the bars are relatively close in height). For the variable regular, the category "watched once per week or greater" has far more observations than "rarely watched." The former category has more than triple the amount of the latter. We will be aware of this disparity in our analysis and continue with caution towards potential bias.


Lastly, we want to examine the distributions of site, sex, and age, all variables that relate to a child's background.

```{r visualizingbackground, message=FALSE}

# want to visualize distributions of site, sex, and age

siteplot <- ggplot(sesame, aes(x = factor(site))) +
  geom_bar(fill = "lightblue") +
  labs(title = "Distribution of Site", y = "Count") +
  scale_x_discrete(labels=c("3-5, disadv., inner city", "4, adv., suburb", "adv., rural", "disadv., rural", "disadv., Spanish")) +
  theme_minimal() +
  theme(axis.title.x = element_blank()) +
  geom_text(aes(label = ..count..), stat = "count", vjust = 1.5, colour = "white")

sexplot <- ggplot(sesame, aes(x = factor(sex))) +
  geom_bar(fill = "lightblue") +
  labs(title = "Distribution of Sex", y = "Count") +
  scale_x_discrete(labels=c("Male", "Female")) +
  theme_minimal() +
  theme(axis.title.x = element_blank()) +
  geom_text(aes(label = ..count..), stat = "count", vjust = 1.5, colour = "white")

ageplot <- ggplot(sesame, aes(x = age)) +
  geom_histogram(fill = "lightblue") +
  labs(title = "Distribution of Age", x = "Age in Months", y = "Count") +
  theme_minimal()

siteplot / (sexplot + ageplot)

```

Distribution of Site and Sex are both categorical variables. Distribution of Site has four categories with roughly the same amount of children (ranging from 43 to 64), but one category with far fewer observations (disadvantaged Spanish-speaking). This cateogry has less than half of the observations as the next smallest category, which is a relatively large disparity. We will continue our analysis with caution towards this bias in the data. The distribution of sex is very even - the male category has 115 observations, while the female category has 125 observations. 
Age is a numerical variable that appears to be normal and bimodal, with two peaks around 50 and 56. There do not appear to be any extreme outliers in the distribution of age.

## Q.1 Prediction Question: Can we use linear regression to predict the change in a child's test scores that occur after watching Sesame street (or in some instances, not watching Seasame street)?

### Linear Regression Models

Here, I am fitting 7 linear regression models. The first 6 models predict a different difference in test score while the last model predicts the total difference in test scores. 

```{r factor-categoricals}

sesame$site <- as.factor(sesame$site)
sesame$sex <- as.factor(sesame$sex)
sesame$viewcat <- as.factor(sesame$viewcat)
sesame$setting <- as.factor(sesame$setting)
sesame$viewenc <- as.factor(sesame$viewenc)

```

```{r create-new-variable}

sesame <- sesame %>%
  mutate(totDiff = bodyDiff + letDiff + formDiff + numbDiff + relatDiff + clasfDiff)

```

```{r}

# Test-Train Split

set.seed(1)
train <- sample(1:nrow(sesame), nrow(sesame)*0.7)

```

```{r}

lin.mod1.full <- lm(bodyDiff ~ (site + sex + age + viewcat + setting + viewenc)^2, data = sesame)
summary(lin.mod1.full)
AIC(lin.mod1.full)

yhat <- predict(lin.mod1.full, newdata = sesame[-train,])
y.test <- sesame[-train, "bodyDiff"]

# Test MSE
mean((yhat-y.test)^2)

```

```{r LASSO}

# Split data between x and y
x <- model.matrix(bodyDiff ~ site + sex + age + viewcat + setting + viewenc,sesame)[,-1]
y <- sesame$bodyDiff

# set seed
set.seed(1)

# cross validation for lambda
# I couldn't run cv.glmnet, so I commented it out, make sure to uncomment these calls.
# cv.out <- cv.glmnet(x, y, alpha = 1) # setting alpha = 1 indicates lasso regression

# optimal lambda value
#best.lam <- cv.out$lambda.min

# lasso regression model with optimal lambda
#las.mod <- glmnet(x, y, alpha = 1, lambda = best.lam)

```

```{r}

lin.mod2.full <- lm(letDiff ~ (site + sex + age + viewcat + setting + viewenc)^2, data = sesame)
summary(lin.mod2.full)
AIC(lin.mod2.full)

yhat <- predict(lin.mod2.full, newdata = sesame[-train,])
y.test <- sesame[-train, "letDiff"]

# Test MSE
mean((yhat-y.test)^2)

```

```{r}

lin.mod3.full <- lm(formDiff ~ (site + sex + age + viewcat + setting + viewenc)^2, data = sesame)
summary(lin.mod3.full)
AIC(lin.mod3.full)

yhat <- predict(lin.mod3.full, newdata = sesame[-train,])
y.test <- sesame[-train, "formDiff"]

# Test MSE
mean((yhat-y.test)^2)

```

```{r}

lin.mod4.full <- lm(numbDiff ~ (site + sex + age + viewcat + setting + viewenc)^2, data = sesame)
summary(lin.mod4.full)
AIC(lin.mod4.full)

yhat <- predict(lin.mod4.full, newdata = sesame[-train,])
y.test <- sesame[-train, "numbDiff"]

# Test MSE
mean((yhat-y.test)^2)

```

```{r}

lin.mod5.full <- lm(relatDiff ~ (site + sex + age + viewcat + setting + viewenc)^2, data = sesame)
summary(lin.mod5.full)
AIC(lin.mod5.full)

yhat <- predict(lin.mod5.full, newdata = sesame[-train,])
y.test <- sesame[-train, "relatDiff"]

# Test MSE
mean((yhat-y.test)^2)

```

```{r}

lin.mod6.full <- lm(clasfDiff ~ (site + sex + age + viewcat + setting + viewenc)^2, data = sesame)
summary(lin.mod6.full)
AIC(lin.mod6.full)

yhat <- predict(lin.mod6.full, newdata = sesame[-train,])
y.test <- sesame[-train, "clasfDiff"]

# Test MSE
mean((yhat-y.test)^2)

```

```{r}

lin.mod7.full <- lm(totDiff ~ (site + sex + age + viewcat + setting + viewenc)^2, data = sesame)
summary(lin.mod7.full)
AIC(lin.mod7.full)

yhat <- predict(lin.mod7.full, newdata = sesame[-train,])
y.test <- sesame[-train, "totDiff"]

# Test MSE
mean((yhat-y.test)^2)

```

### Regression Tree Models 

In the context of a regression tree, the deviance is simply the sum of the squared errors.

## Model 1

```{r}

set.seed(1)

reg.tree.1 <- tree(bodyDiff ~ site + sex + age + viewcat + setting + viewenc, sesame, subset = train)
summary(reg.tree.1)

plot(reg.tree.1)
text(reg.tree.1, pretty = 0)

cv.reg.tree.1  <- cv.tree(reg.tree.1)
plot(cv.reg.tree.1$size, cv.reg.tree.1$dev, type = "b")

```

```{r}

prune.reg.tree.1  <- prune.tree(reg.tree.1, best = 4)
plot(prune.reg.tree.1)
text(prune.reg.tree.1, pretty = 0)

yhat <- predict(prune.reg.tree.1, newdata = sesame[-train,])
y.test <- sesame[-train, "bodyDiff"]

# Test MSE
mean((yhat-y.test)^2)

```

## Model 2

```{r}

set.seed(1)

reg.tree.2 <- tree(letDiff ~ site + sex + age + viewcat + setting + viewenc, sesame, subset = train)
summary(reg.tree.2)

plot(reg.tree.2)
text(reg.tree.2, pretty = 0)

cv.reg.tree.2  <- cv.tree(reg.tree.2)
plot(cv.reg.tree.2$size, cv.reg.tree.2$dev, type = "b")

```

```{r}

prune.reg.tree.2  <- prune.tree(reg.tree.2, best = 3)
plot(prune.reg.tree.2)
text(prune.reg.tree.2, pretty = 0)

yhat <- predict(prune.reg.tree.2, newdata = sesame[-train,])
y.test <- sesame[-train, "letDiff"]

# Test MSE
mean((yhat-y.test)^2)

```

## Model 3

```{r}

set.seed(1)

reg.tree.3 <- tree(formDiff ~ site + sex + age + viewcat + setting + viewenc, sesame, subset = train)
summary(reg.tree.3)

plot(reg.tree.3)
text(reg.tree.3, pretty = 0)

cv.reg.tree.3  <- cv.tree(reg.tree.3)
plot(cv.reg.tree.3$size, cv.reg.tree.3$dev, type = "b")

```

```{r}

prune.reg.tree.3  <- prune.tree(reg.tree.3, best = 2)
plot(prune.reg.tree.3)
text(prune.reg.tree.3, pretty = 0)

yhat <- predict(prune.reg.tree.3, newdata = sesame[-train,])
y.test <- sesame[-train, "formDiff"]

# Test MSE
mean((yhat-y.test)^2)

```

## Model 4

```{r}

set.seed(1)

reg.tree.4 <- tree(numbDiff ~ site + sex + age + viewcat + setting + viewenc, sesame, subset = train)
summary(reg.tree.4)

plot(reg.tree.4)
text(reg.tree.4, pretty = 0)

cv.reg.tree.4  <- cv.tree(reg.tree.4)
plot(cv.reg.tree.4$size, cv.reg.tree.4$dev, type = "b")

```

```{r}

prune.reg.tree.4  <- prune.tree(reg.tree.4, best = 3)
plot(prune.reg.tree.4)
text(prune.reg.tree.4, pretty = 0)

yhat <- predict(prune.reg.tree.4, newdata = sesame[-train,])
y.test <- sesame[-train, "numbDiff"]

# Test MSE
mean((yhat-y.test)^2)

```

## Model 5

```{r}

set.seed(1)

reg.tree.5 <- tree(relatDiff ~ site + sex + age + viewcat + setting + viewenc, sesame, subset = train)
summary(reg.tree.5)

plot(reg.tree.5)
text(reg.tree.5, pretty = 0)

cv.reg.tree.5  <- cv.tree(reg.tree.5)
plot(cv.reg.tree.5$size, cv.reg.tree.5$dev, type = "b")

```

```{r}

prune.reg.tree.5  <- prune.tree(reg.tree.5, best = 5)
plot(prune.reg.tree.5)
text(prune.reg.tree.5, pretty = 0)

yhat <- predict(prune.reg.tree.5, newdata = sesame[-train,])
y.test <- sesame[-train, "relatDiff"]

# Test MSE
mean((yhat-y.test)^2)

```

## Model 6

```{r}

set.seed(1)

reg.tree.6 <- tree(clasfDiff ~ site + sex + age + viewcat + setting + viewenc, sesame, subset = train)
summary(reg.tree.6)

plot(reg.tree.6)
text(reg.tree.6, pretty = 0)

cv.reg.tree.6  <- cv.tree(reg.tree.6)
plot(cv.reg.tree.6$size, cv.reg.tree.6$dev, type = "b")

```

```{r}

prune.reg.tree.6  <- prune.tree(reg.tree.6, best = 2)
plot(prune.reg.tree.6)
text(prune.reg.tree.6, pretty = 0)

yhat <- predict(prune.reg.tree.6, newdata = sesame[-train,])
y.test <- sesame[-train, "clasfDiff"]

# Test MSE
mean((yhat-y.test)^2)

```

## Model 7

```{r}

set.seed(1)

reg.tree.7 <- tree(totDiff ~ site + sex + age + viewcat + setting + viewenc, sesame, subset = train)
summary(reg.tree.7)

plot(reg.tree.7)
text(reg.tree.7, pretty = 0)

cv.reg.tree.7  <- cv.tree(reg.tree.7)
plot(cv.reg.tree.7$size, cv.reg.tree.7$dev, type = "b")

```

```{r}

prune.reg.tree.7  <- prune.tree(reg.tree.7, best = 2)
plot(prune.reg.tree.7)
text(prune.reg.tree.7, pretty = 0)

yhat <- predict(prune.reg.tree.7, newdata = sesame[-train,])
y.test <- sesame[-train, "totDiff"]

# Test MSE
mean((yhat-y.test)^2)

```

## Q.2 Classification Question: Can we use the pre-test scores and other demographic variables to predict which region the children came from?


```{r}
ggplot(data=sesame, aes(x=prenumb, y=prelet, color=site))+
  geom_point()
ggplot(data=sesame, aes(x=prenumb, y=prerelat, color=site))+
  geom_point()
ggplot(data=sesame, aes(x=preform, y=preclasf, color=site))+
  geom_point()
ggplot(data=sesame, aes(x=preclasf, y=prerelat, color=site))+
  geom_point()
```



### SVM

```{r test-train-split}
set.seed(3241)

n <- nrow(sesame)
train.index <- sample(1:n, size = floor(0.7*n), replace=FALSE)
train.data <- sesame.sd[train.index,]
test.data <- sesame.sd[-train.index,]

train.data %>%
  count(site)
```

```{r svm-fitting}
#1	60			
#2	55			
#3	64			
#4	43			
#5	18	

total.weight <- 60+55+64+43+18
weight.1 <- total.weight/(5*60)
weight.2 <- total.weight/(5*55)
weight.3 <- total.weight/(5*64)  
weight.4 <- total.weight/(5*43)  
weight.5 <- total.weight/(5*18)  


weight.4 <- 1.5
weight.5 <- 3
# Response: site (categorical)
set.seed(315)
costs <- c(0.001, 0.01, 0.1, 1, 5, 10, 100)
# c(0.1, 0.2, 0.5, 0.7, 1, 2, 3, 4)
gammas <- seq(0, 4, by=0.1)

linear.tune <- tune(svm, site~female+ male + sd_age+sd_pBod+sd_plet+sd_pform + sd_pnumb+sd_prelat+sd_pclasf+sd_peabody, 
                    data=train.data, kernel="linear",
                    ranges=list(cost=costs),
                    class.weights=c("1"=weight.1,
                                    "2"=weight.2,
                                    "3"=weight.3,
                                    "4"=weight.4,
                                    "5"=weight.5),
                    class.type="one.versus.one")

radial.tune <- tune(svm, site~female + male + sd_age+sd_pBod+sd_plet+sd_pform + sd_pnumb+sd_prelat+sd_pclasf+sd_peabody, 
                    data=train.data, kernel="radial",
                    ranges=list(cost=costs, 
                                gamma=gammas),
                    class.weights=c("1"=weight.1,
                                    "2"=weight.2,
                                    "3"=weight.3,
                                    "4"=weight.4,
                                    "5"=weight.5))
#radial.tune <- tune(svm, site~sex+age+prebody+prelet+preform+prenumb+prerelat+preclasf, 
#                    data=train.data, kernel="radial",
#                    ranges=list(cost=costs, 
#                                gamma=gammas))

sigmoid.tune <- tune(svm, site~female + male + sd_age+sd_pBod+sd_plet+sd_pform + sd_pnumb+sd_prelat+sd_pclasf+sd_peabody, 
                    data=train.data, kernel="sigmoid",
                    ranges=list(cost=costs, 
                                gamma=gammas),
                    class.weights=c("1"=weight.1,
                                    "2"=weight.2,
                                    "3"=weight.3,
                                    "4"=weight.4,
                                    "5"=weight.5))
```

```{r}
linear.conMatrix <- table(true=test.data[, "site"],
                          pred=predict(linear.tune$best.model, newdata=test.data))

radial.conMatrix <- table(true=test.data[, "site"],
                          pred=predict(radial.tune$best.model, newdata=test.data))

sigmoid.conMatrix <- table(true=test.data[, "site"],
                          pred=predict(sigmoid.tune$best.model, newdata=test.data))

confusionMatrix(linear.conMatrix)
confusionMatrix(radial.conMatrix)
confusionMatrix(sigmoid.conMatrix)

predict(linear.tune$best.model, newdata=test.data)
predict(radial.tune$best.model, newdata=test.data)
predict(sigmoid.tune$best.model, newdata=test.data)
test.data$site
```
Radial kernel improves prediction on class 1. 

RBF slightly improved after standardizing? (it seems slightly more likely to predict
on class 1. ) thought, simpler models still retain the same performance (arguably better)
sd_age+sd_pBod+sd_plet. But we are still not getting any prediction on class 4 & 5. 

After assign class weights using this formula:
$$
w_j =\frac{n}{kn_j} , \text{ n is total number of data points, k is number of classes}
$$
Our model begins to make predictions on class 4 & class 5, though at the cost of 
overall accuracy. If we increase the weight for 4 & 5 to 1.5 and 3 respectively
the performance of Radial SVM decreases but that of linear SVM increases to be comparable
to Radial SVM's recorded highest accuracy (a little bit over 0.40).


```{r}
# trying to do more EDA to see if anything explains why the data is not linearly separable
boxplot(prebody~site, data=sesame)
boxplot(prelet~site, data=sesame)
boxplot(preform~site, data=sesame)
boxplot(prenumb~site, data=sesame)
boxplot(prerelat~site, data=sesame)
boxplot(preclasf~site, data=sesame)
boxplot(peabody~site, data=sesame)
```


### Trees

```{r selecting-features-trees}

set.seed(3215)
#tree.data <- sesame %>%
#  select(site, sex, age, viewcat, setting, viewenc, prebody, prelet, preform, 
#         prenumb, prerelat, preclasf)

n <- nrow(sesame)
train.index <- sample(1:n, size = floor(0.7*n), replace=FALSE)
#train.tree <- tree.data[train.index,]
#test.tree <- tree.data[-train.index,]
# "viewcat", "setting", "viewenc",

# ,"prebody", "prelet","preform", "prenumb", "prerelat", "preclasf", "postbody", "postlet", "postform", #"postnumb", "postrelat", "postclasf", "peabody"
                   
tree.features <- c("site", "age", "viewcat", "setting", "viewenc")

tree.data <- sesame[, tree.features]
train.data <- tree.data[train.index,]
test.data <- tree.data[-train.index,]

```

```{r fitting-randomForest}

rf.tree<- randomForest(site~., data=tree.data, subset=train.index,
                       mtry=4, importance=TRUE)

importance(rf.tree)

rf.pred <- predict(rf.tree, newdata=test.data)

tree.conMatrix <- table(true=test.data[,"site"],
                         pred=rf.pred)
confusionMatrix(tree.conMatrix)

varImpPlot(rf.tree)

```
0.42 -- 0.5139 (but not including the test scores.)
around 0.45-0.48, when including the pretest scores.

As seen in the table above, there is a notable discrepancy in the number of observations that lay in classes 4 and 5 for the variable ``site``. More specifically, in the training data, there are just 25 observations with a value of 4 for the variable ``site`` and just 13 observations with a value of 5 for the variable ``site``. In other words, there are less disadvantaged rural children and disadvantaged Spanish speaking children. 

Consequently, when we initially ran our random forest model, our model was performing worse for test observations that take on the values 4 or 5 for the variable ``site``.

To remedy this problem, we decided to use Synthetic Minority Oversampling Technique (SMOTE). SMOTE works by generating new samples in the classes of the response variable that are less represented. These new samples are generated using linear combinations of the "k" nearest neighbors in a given class. In this instance, we set $k=5$. 

```{r}

train.data$age <- as.numeric(train.data$age)
train.data$viewcat <- as.numeric(train.data$viewcat)
train.data$setting <- as.numeric(train.data$setting)
train.data$viewenc <- as.factor(train.data$viewenc)

test.data$age <- as.numeric(test.data$age)
test.data$viewcat <- as.numeric(test.data$viewcat)
test.data$setting <- as.numeric(test.data$setting)
test.data$viewenc <- as.factor(test.data$viewenc)

balanced.train.data <- SmoteClassif(site ~ ., train.data, k = 5, repl = FALSE, dist = "HEOM")
  # k --> represents the number of nearest neighbors (5) used to generate new examples of the minority class
  # repl = FALSE --> cannot have repetition of examples when performing under-sampling by selecting among the majority class(es) examples

balanced.train.data %>%
  count(site)

```

```{r re-fitting-randomForest}

rf.tree<- randomForest(site~., data=balanced.train.data,
                       mtry=4, importance=TRUE)

importance(rf.tree)

rf.pred <- predict(rf.tree, newdata=test.data)

tree.conMatrix <- table(true=test.data[,"site"],
                         pred=rf.pred)
confusionMatrix(tree.conMatrix)

```

While the ``SmoteClassif()`` function certainly did its job by balancing out the number of observations for each value of ``site`` in the training data, the new random forest model (fitted to this new data set) is less accurate and sees little improvement in the detection of ``site`` values of 4 and 5. 

```{r fitting-boosting}

set.seed(3215)

# ,"prebody", "prelet","preform", "prenumb", "prerelat", "preclasf", "postbody", "postlet", "postform", #"postnumb", "postrelat", "postclasf", "peabody"
                   
                   

features <- c("site", "age", "viewcat", "setting", "viewenc","prebody", "prelet","preform", "prenumb", "prerelat", "preclasf", "postbody", "postlet", "postform", "postnumb", "postrelat", "postclasf", "peabody")

tree.2 <- sesame[, features]
train.2 <- tree.2[train.index,]
test.2 <- tree.2[-train.index,]

boost.tree <- gbm(site ~., data=train.2,
                  distribution="multinomial", n.trees=5000,
                  interaction.depth=1)

#y.boost <- table(true=test.2[,"site"],
#                 pred=predict(boost.tree, newdata=test.2))

boost.conMatrix <- table(true=test.2$site,
                         pred=predict(boost.tree, newdata=test.2))
confusionMatrix(boost.conMatrix)

```



### Questions for OH:
## anything else needed in EDA?
## Both linear and radial kernels never output predictions for 4 & 5?
## polynomial kernel? Which variables to give polynomial terms
## use PCA to perform feature selection?
## feature selections for SVM in general? 

## how to interpret the confusion matrix tables for SVM & Trees
## How to interpret the imporatnce variance for multiclass classification
## interpretations about the dataset, using the bad performance of the classifiers


